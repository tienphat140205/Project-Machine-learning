{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T14:09:40.713118Z",
     "iopub.status.busy": "2025-05-21T14:09:40.712851Z",
     "iopub.status.idle": "2025-05-21T14:09:40.896437Z",
     "shell.execute_reply": "2025-05-21T14:09:40.895709Z",
     "shell.execute_reply.started": "2025-05-21T14:09:40.713096Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ECRTM' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/BobXWu/ECRTM.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-21T14:09:40.898162Z",
     "iopub.status.busy": "2025-05-21T14:09:40.897910Z",
     "iopub.status.idle": "2025-05-21T14:09:40.910957Z",
     "shell.execute_reply": "2025-05-21T14:09:40.910391Z",
     "shell.execute_reply.started": "2025-05-21T14:09:40.898137Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Số mẫu train: 10000\n",
      "📊 Số nhãn train: 10000\n",
      "📊 Số mẫu test: 2500\n",
      "📊 Số nhãn test: 2500\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/kaggle/working/ECRTM/data/AGNews\"\n",
    "\n",
    "with open(f\"{dataset_path}/train_texts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_train = f.read().splitlines()\n",
    "\n",
    "with open(f\"{dataset_path}/train_labels.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    label_train = f.read().splitlines()\n",
    "\n",
    "with open(f\"{dataset_path}/test_texts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_test = f.read().splitlines()\n",
    "\n",
    "with open(f\"{dataset_path}/test_labels.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    label_test = f.read().splitlines()\n",
    "\n",
    "print(\"📊 Số mẫu train:\", len(text_train))\n",
    "print(\"📊 Số nhãn train:\", len(label_train))\n",
    "print(\"📊 Số mẫu test:\", len(text_test))\n",
    "print(\"📊 Số nhãn test:\", len(label_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T14:09:40.911974Z",
     "iopub.status.busy": "2025-05-21T14:09:40.911739Z",
     "iopub.status.idle": "2025-05-21T14:09:46.717323Z",
     "shell.execute_reply": "2025-05-21T14:09:46.716366Z",
     "shell.execute_reply.started": "2025-05-21T14:09:40.911958Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Epoch 1/4, Loss: 0.7328\n",
      "📈 Epoch 2/4, Loss: 0.2328\n",
      "📈 Epoch 3/4, Loss: 0.1280\n",
      "📈 Epoch 4/4, Loss: 0.0640\n",
      "\n",
      "✅ Đánh giá trên tập test:\n",
      "🎯 Accuracy :  0.8528\n",
      "🎯 Precision:  0.8534\n",
      "🎯 Recall   :  0.8528\n",
      "🎯 F1-score :  0.8530\n",
      "\n",
      "📋 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84       607\n",
      "           1       0.94      0.92      0.93       627\n",
      "           2       0.82      0.81      0.82       626\n",
      "           3       0.83      0.82      0.83       640\n",
      "\n",
      "    accuracy                           0.85      2500\n",
      "   macro avg       0.85      0.85      0.85      2500\n",
      "weighted avg       0.85      0.85      0.85      2500\n",
      "\n",
      "\n",
      "💾 Đã lưu mô hình vào text_classifier.pth\n",
      "\n",
      "📝 Input: How does inflation affect small businesses?\n",
      "🔍 Predicted topic: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=20000)\n",
    "X_train_vec = vectorizer.fit_transform(text_train)\n",
    "X_test_vec = vectorizer.transform(text_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_enc = label_encoder.fit_transform(label_train)\n",
    "y_test_enc = label_encoder.transform(label_test)\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train_vec.toarray())\n",
    "y_train_tensor = torch.LongTensor(y_train_enc)\n",
    "X_test_tensor = torch.FloatTensor(X_test_vec.toarray())\n",
    "y_test_tensor = torch.LongTensor(y_test_enc)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "input_dim = X_train_vec.shape[1]\n",
    "hidden_dim = 128\n",
    "output_dim = len(label_encoder.classes_)\n",
    "model = SimpleNN(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"📈 Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# --- Metrics ---\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(\"\\n✅ Đánh giá trên tập test:\")\n",
    "print(f\"🎯 Accuracy :  {accuracy:.4f}\")\n",
    "print(f\"🎯 Precision:  {precision:.4f}\")\n",
    "print(f\"🎯 Recall   :  {recall:.4f}\")\n",
    "print(f\"🎯 F1-score :  {f1:.4f}\")\n",
    "print(\"\\n📋 Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vectorizer': vectorizer,\n",
    "    'label_encoder': label_encoder\n",
    "}, 'text_classifier.pth')\n",
    "print(\"\\n💾 Đã lưu mô hình vào text_classifier.pth\")\n",
    "\n",
    "def predict(text):\n",
    "    model.eval()\n",
    "    vec = vectorizer.transform([text])\n",
    "    input_tensor = torch.FloatTensor(vec.toarray()).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "    return label_encoder.inverse_transform(pred.cpu().numpy())[0]\n",
    "\n",
    "example_text = \"How does inflation affect small businesses?\"\n",
    "predicted_label = predict(example_text)\n",
    "print(f\"\\n📝 Input: {example_text}\")\n",
    "print(f\"🔍 Predicted topic: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T14:09:46.719765Z",
     "iopub.status.busy": "2025-05-21T14:09:46.719440Z",
     "iopub.status.idle": "2025-05-21T14:09:46.970636Z",
     "shell.execute_reply": "2025-05-21T14:09:46.969916Z",
     "shell.execute_reply.started": "2025-05-21T14:09:46.719747Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Đánh giá mô hình Naive Bayes:\n",
      "🔹 Accuracy       : 0.8788\n",
      "🔹 Precision (weighted): 0.8784\n",
      "🔹 Recall    (weighted): 0.8788\n",
      "🔹 F1-score  (weighted): 0.8786\n",
      "🔹 F1-score    (macro): 0.8788\n",
      "\n",
      "📋 Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89       607\n",
      "           1       0.94      0.96      0.95       627\n",
      "           2       0.83      0.83      0.83       626\n",
      "           3       0.85      0.85      0.85       640\n",
      "\n",
      "    accuracy                           0.88      2500\n",
      "   macro avg       0.88      0.88      0.88      2500\n",
      "weighted avg       0.88      0.88      0.88      2500\n",
      "\n",
      "\n",
      "📝 Input: How does inflation affect small businesses?\n",
      "🔍 Predicted topic: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_enc = label_encoder.fit_transform(label_train)\n",
    "y_test_enc = label_encoder.transform(label_test)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10000)\n",
    "X_train_vec = vectorizer.fit_transform(text_train)\n",
    "X_test_vec = vectorizer.transform(text_test)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vec, y_train_enc)\n",
    "\n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "label_names = label_encoder.classes_\n",
    "\n",
    "print(\"\\n✅ Đánh giá mô hình Naive Bayes:\")\n",
    "print(f\"🔹 Accuracy       : {accuracy_score(y_test_enc, y_pred):.4f}\")\n",
    "print(f\"🔹 Precision (weighted): {precision_score(y_test_enc, y_pred, average='weighted'):.4f}\")\n",
    "print(f\"🔹 Recall    (weighted): {recall_score(y_test_enc, y_pred, average='weighted'):.4f}\")\n",
    "print(f\"🔹 F1-score  (weighted): {f1_score(y_test_enc, y_pred, average='weighted'):.4f}\")\n",
    "print(f\"🔹 F1-score    (macro): {f1_score(y_test_enc, y_pred, average='macro'):.4f}\")\n",
    "\n",
    "print(\"\\n📋 Classification Report:\\n\")\n",
    "print(classification_report(y_test_enc, y_pred, target_names=label_names))\n",
    "\n",
    "def predict_topic(text):\n",
    "    vec = vectorizer.transform([text])\n",
    "    pred = model.predict(vec)[0]\n",
    "    return label_encoder.inverse_transform([pred])[0]\n",
    "\n",
    "example = \"How does inflation affect small businesses?\"\n",
    "print(\"\\n📝 Input:\", example)\n",
    "print(\"🔍 Predicted topic:\", predict_topic(example))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T14:09:46.971534Z",
     "iopub.status.busy": "2025-05-21T14:09:46.971250Z",
     "iopub.status.idle": "2025-05-21T14:09:49.359998Z",
     "shell.execute_reply": "2025-05-21T14:09:49.359287Z",
     "shell.execute_reply.started": "2025-05-21T14:09:46.971517Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy: 0.8828\n",
      "\n",
      "✅ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Topic 0       0.88      0.89      0.89       607\n",
      "     Topic 1       0.93      0.96      0.95       627\n",
      "     Topic 2       0.86      0.83      0.85       626\n",
      "     Topic 3       0.85      0.85      0.85       640\n",
      "\n",
      "    accuracy                           0.88      2500\n",
      "   macro avg       0.88      0.88      0.88      2500\n",
      "weighted avg       0.88      0.88      0.88      2500\n",
      "\n",
      "\n",
      "📌 Predicted topic: Topic 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10000)\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(text_train)\n",
    "X_test_vec = vectorizer.transform(text_test)\n",
    "\n",
    "y_train = list(map(int, label_train))\n",
    "y_test = list(map(int, label_test))\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, solver='lbfgs') \n",
    "lr_model.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = lr_model.predict(X_test_vec)\n",
    "\n",
    "label_names = [f\"Topic {i}\" for i in sorted(set(y_train))]\n",
    "\n",
    "print(\"✅ Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n✅ Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_names))\n",
    "\n",
    "def predict_topic(text):\n",
    "    vec = vectorizer.transform([text])\n",
    "    pred = lr_model.predict(vec)[0]\n",
    "    return label_names[int(pred)]\n",
    "\n",
    "example = \"How does inflation affect small businesses?\"\n",
    "print(\"\\n📌 Predicted topic:\", predict_topic(example))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SGD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T14:09:49.360784Z",
     "iopub.status.busy": "2025-05-21T14:09:49.360597Z",
     "iopub.status.idle": "2025-05-21T14:09:49.667614Z",
     "shell.execute_reply": "2025-05-21T14:09:49.666905Z",
     "shell.execute_reply.started": "2025-05-21T14:09:49.360769Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy (SGD): 0.8828\n",
      "\n",
      "📋 Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88       607\n",
      "           1       0.93      0.96      0.95       627\n",
      "           2       0.87      0.83      0.85       626\n",
      "           3       0.85      0.86      0.85       640\n",
      "\n",
      "    accuracy                           0.88      2500\n",
      "   macro avg       0.88      0.88      0.88      2500\n",
      "weighted avg       0.88      0.88      0.88      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10000)\n",
    "X_train_vec = vectorizer.fit_transform(text_train)\n",
    "X_test_vec = vectorizer.transform(text_test)\n",
    "\n",
    "y_train = list(map(int, label_train))\n",
    "y_test = list(map(int, label_test))\n",
    "\n",
    "sgd_model = SGDClassifier(\n",
    "    loss=\"log_loss\",   \n",
    "    random_state=0,\n",
    "    n_jobs=-1           \n",
    ")\n",
    "sgd_model.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = sgd_model.predict(X_test_vec)\n",
    "\n",
    "\n",
    "\n",
    "print(\"✅ Accuracy (SGD):\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n📋 Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T14:09:49.668853Z",
     "iopub.status.busy": "2025-05-21T14:09:49.668569Z",
     "iopub.status.idle": "2025-05-21T14:10:15.828409Z",
     "shell.execute_reply": "2025-05-21T14:10:15.827602Z",
     "shell.execute_reply.started": "2025-05-21T14:09:49.668829Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8908\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89       607\n",
      "           1       0.94      0.96      0.95       627\n",
      "           2       0.88      0.84      0.86       626\n",
      "           3       0.85      0.87      0.86       640\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.89      0.89      0.89      2500\n",
      "weighted avg       0.89      0.89      0.89      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10000)\n",
    "X_train_vec = vectorizer.fit_transform(text_train)\n",
    "X_test_vec = vectorizer.transform(text_test)\n",
    "\n",
    "y_train = list(map(int, label_train))\n",
    "y_test = list(map(int, label_test))\n",
    "\n",
    "svc_model = SVC(kernel='rbf', gamma='scale') \n",
    "svc_model.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = svc_model.predict(X_test_vec)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T14:10:15.829681Z",
     "iopub.status.busy": "2025-05-21T14:10:15.829405Z",
     "iopub.status.idle": "2025-05-21T14:10:21.746753Z",
     "shell.execute_reply": "2025-05-21T14:10:21.746061Z",
     "shell.execute_reply.started": "2025-05-21T14:10:15.829654Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy (Random Forest): 0.8392\n",
      "\n",
      "📋 Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Topic 0       0.85      0.85      0.85       607\n",
      "     Topic 1       0.90      0.93      0.91       627\n",
      "     Topic 2       0.82      0.78      0.80       626\n",
      "     Topic 3       0.79      0.80      0.80       640\n",
      "\n",
      "    accuracy                           0.84      2500\n",
      "   macro avg       0.84      0.84      0.84      2500\n",
      "weighted avg       0.84      0.84      0.84      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10000)\n",
    "X_train_vec = vectorizer.fit_transform(text_train)\n",
    "X_test_vec = vectorizer.transform(text_test)\n",
    "\n",
    "y_train = list(map(int, label_train))\n",
    "y_test = list(map(int, label_test))\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,        \n",
    "    max_depth=None,          \n",
    "    min_samples_split=2,     \n",
    "    random_state=42,\n",
    "    n_jobs=-1                \n",
    ")\n",
    "rf_model.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test_vec)\n",
    "\n",
    "print(\"✅ Accuracy (Random Forest):\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n📋 Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T14:10:21.747840Z",
     "iopub.status.busy": "2025-05-21T14:10:21.747568Z",
     "iopub.status.idle": "2025-05-21T14:10:21.958828Z",
     "shell.execute_reply": "2025-05-21T14:10:21.958040Z",
     "shell.execute_reply.started": "2025-05-21T14:10:21.747816Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy: 0.8552\n",
      "\n",
      "📄 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8553    0.8764    0.8657       607\n",
      "           1     0.9244    0.9362    0.9303       627\n",
      "           2     0.8294    0.7843    0.8062       626\n",
      "           3     0.8111    0.8250    0.8180       640\n",
      "\n",
      "    accuracy                         0.8552      2500\n",
      "   macro avg     0.8550    0.8555    0.8551      2500\n",
      "weighted avg     0.8548    0.8552    0.8548      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10000)\n",
    "X_train_vec = vectorizer.fit_transform(text_train)\n",
    "X_test_vec = vectorizer.transform(text_test)\n",
    "\n",
    "label_train_int = np.array(label_train, dtype=int)\n",
    "label_test_int = np.array(label_test, dtype=int)\n",
    "available_labels = np.unique(label_train_int)\n",
    "\n",
    "centroids = []\n",
    "for label in available_labels:\n",
    "    group_indices = np.where(label_train_int == label)[0]\n",
    "    group_vectors = X_train_vec[group_indices]\n",
    "    centroid = group_vectors.mean(axis=0)\n",
    "    centroids.append(centroid)\n",
    "\n",
    "centroids_matrix = np.asarray(np.vstack(centroids))\n",
    "similarities = cosine_similarity(X_test_vec, centroids_matrix)\n",
    "y_pred = available_labels[np.argmax(similarities, axis=1)]\n",
    "\n",
    "\n",
    "print(\"✅ Accuracy:\", accuracy_score(label_test_int, y_pred))\n",
    "print(\"\\n📄 Classification Report:\\n\", classification_report(label_test_int, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROLDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T14:10:21.961194Z",
     "iopub.status.busy": "2025-05-21T14:10:21.960932Z",
     "iopub.status.idle": "2025-05-21T14:10:21.973239Z",
     "shell.execute_reply": "2025-05-21T14:10:21.972486Z",
     "shell.execute_reply.started": "2025-05-21T14:10:21.961177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ProdLDA(nn.Module):\n",
    "    '''\n",
    "        Autoencoding Variational Inference For Topic Models. ICLR 2017\n",
    "\n",
    "        Akash Srivastava, Charles Sutton.\n",
    "    '''\n",
    "    def __init__(self, vocab_size, num_topics=50, en_units=200, dropout=0.4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_topics = num_topics\n",
    "\n",
    "        self.a = 1 * np.ones((1, num_topics)).astype(np.float32)\n",
    "        self.mu2 = nn.Parameter(torch.as_tensor((np.log(self.a).T - np.mean(np.log(self.a), 1)).T))\n",
    "        self.var2 = nn.Parameter(torch.as_tensor((((1.0 / self.a) * (1 - (2.0 / num_topics))).T + (1.0 / (num_topics * num_topics)) * np.sum(1.0 / self.a, 1)).T))\n",
    "\n",
    "        self.mu2.requires_grad = False\n",
    "        self.var2.requires_grad = False\n",
    "\n",
    "        self.fc11 = nn.Linear(vocab_size, en_units)\n",
    "        self.fc12 = nn.Linear(en_units, en_units)\n",
    "        self.fc21 = nn.Linear(en_units, num_topics)\n",
    "        self.fc22 = nn.Linear(en_units, num_topics)\n",
    "\n",
    "        self.mean_bn = nn.BatchNorm1d(num_topics, eps=0.001, momentum=0.001, affine=True)\n",
    "        self.mean_bn.weight.data.copy_(torch.ones(num_topics))\n",
    "        self.mean_bn.weight.requires_grad = False\n",
    "\n",
    "        self.logvar_bn = nn.BatchNorm1d(num_topics, eps=0.001, momentum=0.001, affine=True)\n",
    "        self.logvar_bn.weight.data.copy_(torch.ones(num_topics))\n",
    "        self.logvar_bn.weight.requires_grad = False\n",
    "\n",
    "        self.decoder_bn = nn.BatchNorm1d(vocab_size, eps=0.001, momentum=0.001, affine=True)\n",
    "        self.decoder_bn.weight.data.copy_(torch.ones(vocab_size))\n",
    "        self.decoder_bn.weight.requires_grad = False\n",
    "\n",
    "        self.fc1_drop = nn.Dropout(dropout)\n",
    "        self.theta_drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.fcd1 = nn.Linear(num_topics, vocab_size, bias=False)\n",
    "        nn.init.xavier_uniform_(self.fcd1.weight)\n",
    "\n",
    "    def get_beta(self):\n",
    "        return self.fcd1.weight.T\n",
    "\n",
    "    def get_theta(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        theta = F.softmax(z, dim=1)\n",
    "        theta = self.theta_drop(theta)\n",
    "        if self.training:\n",
    "            return theta, mu, logvar\n",
    "        else:\n",
    "            return theta\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + (eps * std)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def encode(self, x):\n",
    "        e1 = F.softplus(self.fc11(x))\n",
    "        e1 = F.softplus(self.fc12(e1))\n",
    "        e1 = self.fc1_drop(e1)\n",
    "        return self.mean_bn(self.fc21(e1)), self.logvar_bn(self.fc22(e1))\n",
    "\n",
    "    def decode(self, theta):\n",
    "        d1 = F.softmax(self.decoder_bn(self.fcd1(theta)), dim=1)\n",
    "        return d1\n",
    "\n",
    "    def forward(self, x):\n",
    "        theta, mu, logvar = self.get_theta(x)\n",
    "        recon_x = self.decode(theta)\n",
    "        loss = self.loss_function(x, recon_x, mu, logvar)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def loss_function(self, x, recon_x, mu, logvar):\n",
    "        recon_loss = -(x * (recon_x + 1e-10).log()).sum(axis=1)\n",
    "        var = logvar.exp()\n",
    "        var_division = var / self.var2\n",
    "        diff = mu - self.mu2\n",
    "        diff_term = diff * diff / self.var2\n",
    "        logvar_division = self.var2.log() - logvar\n",
    "        KLD = 0.5 * ((var_division + diff_term + logvar_division).sum(axis=1) - self.num_topics)\n",
    "        loss = (recon_loss + KLD).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T14:10:21.974111Z",
     "iopub.status.busy": "2025-05-21T14:10:21.973892Z",
     "iopub.status.idle": "2025-05-21T14:10:22.525273Z",
     "shell.execute_reply": "2025-05-21T14:10:22.524498Z",
     "shell.execute_reply.started": "2025-05-21T14:10:21.974084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import os\n",
    "train_bow_path = os.path.join(dataset_path, \"train_bow.npz\")\n",
    "test_bow_path = os.path.join(dataset_path, \"test_bow.npz\")\n",
    "\n",
    "X_train_sparse = sparse.load_npz(train_bow_path)\n",
    "X_test_sparse = sparse.load_npz(test_bow_path)\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train_sparse.toarray())\n",
    "X_test_tensor = torch.FloatTensor(X_test_sparse.toarray())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "X_test_tensor = X_test_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T14:10:22.526303Z",
     "iopub.status.busy": "2025-05-21T14:10:22.526078Z",
     "iopub.status.idle": "2025-05-21T14:11:51.223547Z",
     "shell.execute_reply": "2025-05-21T14:11:51.222857Z",
     "shell.execute_reply.started": "2025-05-21T14:10:22.526285Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 30755.5206\n",
      "Epoch 2, Loss: 26514.2245\n",
      "Epoch 3, Loss: 26319.0188\n",
      "Epoch 4, Loss: 26135.6382\n",
      "Epoch 5, Loss: 25972.4401\n",
      "Epoch 6, Loss: 25820.2441\n",
      "Epoch 7, Loss: 25683.7093\n",
      "Epoch 8, Loss: 25562.3176\n",
      "Epoch 9, Loss: 25453.1503\n",
      "Epoch 10, Loss: 25350.7052\n",
      "Epoch 11, Loss: 25254.2374\n",
      "Epoch 12, Loss: 25168.0053\n",
      "Epoch 13, Loss: 25080.9723\n",
      "Epoch 14, Loss: 25010.5262\n",
      "Epoch 15, Loss: 24955.3282\n",
      "Epoch 16, Loss: 24895.0938\n",
      "Epoch 17, Loss: 24843.3042\n",
      "Epoch 18, Loss: 24797.2644\n",
      "Epoch 19, Loss: 24762.1695\n",
      "Epoch 20, Loss: 24730.8981\n",
      "Epoch 21, Loss: 24695.1783\n",
      "Epoch 22, Loss: 24676.4764\n",
      "Epoch 23, Loss: 24646.6774\n",
      "Epoch 24, Loss: 24624.9722\n",
      "Epoch 25, Loss: 24601.0133\n",
      "Epoch 26, Loss: 24579.1179\n",
      "Epoch 27, Loss: 24562.6606\n",
      "Epoch 28, Loss: 24547.2502\n",
      "Epoch 29, Loss: 24524.0017\n",
      "Epoch 30, Loss: 24507.0742\n",
      "Epoch 31, Loss: 24506.9647\n",
      "Epoch 32, Loss: 24497.1184\n",
      "Epoch 33, Loss: 24474.8368\n",
      "Epoch 34, Loss: 24470.1998\n",
      "Epoch 35, Loss: 24470.5223\n",
      "Epoch 36, Loss: 24458.6033\n",
      "Epoch 37, Loss: 24441.2555\n",
      "Epoch 38, Loss: 24442.6481\n",
      "Epoch 39, Loss: 24437.5059\n",
      "Epoch 40, Loss: 24439.6972\n",
      "Epoch 41, Loss: 24416.5860\n",
      "Epoch 42, Loss: 24424.1422\n",
      "Epoch 43, Loss: 24411.6446\n",
      "Epoch 44, Loss: 24404.6168\n",
      "Epoch 45, Loss: 24408.2133\n",
      "Epoch 46, Loss: 24408.1619\n",
      "Epoch 47, Loss: 24396.2632\n",
      "Epoch 48, Loss: 24391.2430\n",
      "Epoch 49, Loss: 24394.0861\n",
      "Epoch 50, Loss: 24382.3583\n",
      "Epoch 51, Loss: 24377.4253\n",
      "Epoch 52, Loss: 24387.3659\n",
      "Epoch 53, Loss: 24366.3946\n",
      "Epoch 54, Loss: 24383.6497\n",
      "Epoch 55, Loss: 24363.5836\n",
      "Epoch 56, Loss: 24374.8234\n",
      "Epoch 57, Loss: 24363.7531\n",
      "Epoch 58, Loss: 24363.6029\n",
      "Epoch 59, Loss: 24358.4014\n",
      "Epoch 60, Loss: 24343.6142\n",
      "Epoch 61, Loss: 24361.1830\n",
      "Epoch 62, Loss: 24348.9855\n",
      "Epoch 63, Loss: 24348.5231\n",
      "Epoch 64, Loss: 24354.0515\n",
      "Epoch 65, Loss: 24328.0420\n",
      "Epoch 66, Loss: 24341.8880\n",
      "Epoch 67, Loss: 24338.7469\n",
      "Epoch 68, Loss: 24339.1914\n",
      "Epoch 69, Loss: 24344.1362\n",
      "Epoch 70, Loss: 24329.4565\n",
      "Epoch 71, Loss: 24319.3598\n",
      "Epoch 72, Loss: 24326.0169\n",
      "Epoch 73, Loss: 24321.1756\n",
      "Epoch 74, Loss: 24318.0220\n",
      "Epoch 75, Loss: 24316.7085\n",
      "Epoch 76, Loss: 24311.4780\n",
      "Epoch 77, Loss: 24352.5091\n",
      "Epoch 78, Loss: 24321.5203\n",
      "Epoch 79, Loss: 24308.0650\n",
      "Epoch 80, Loss: 24304.7583\n",
      "Epoch 81, Loss: 24305.4928\n",
      "Epoch 82, Loss: 24294.7780\n",
      "Epoch 83, Loss: 24309.1617\n",
      "Epoch 84, Loss: 24293.5267\n",
      "Epoch 85, Loss: 24274.8894\n",
      "Epoch 86, Loss: 24301.9394\n",
      "Epoch 87, Loss: 24289.3852\n",
      "Epoch 88, Loss: 24288.0884\n",
      "Epoch 89, Loss: 24278.2844\n",
      "Epoch 90, Loss: 24286.4050\n",
      "Epoch 91, Loss: 24305.6743\n",
      "Epoch 92, Loss: 24298.8628\n",
      "Epoch 93, Loss: 24283.4360\n",
      "Epoch 94, Loss: 24289.1253\n",
      "Epoch 95, Loss: 24296.3783\n",
      "Epoch 96, Loss: 24271.0498\n",
      "Epoch 97, Loss: 24293.6756\n",
      "Epoch 98, Loss: 24273.9848\n",
      "Epoch 99, Loss: 24270.9989\n",
      "Epoch 100, Loss: 24268.6063\n",
      "Epoch 101, Loss: 24276.3851\n",
      "Epoch 102, Loss: 24281.5946\n",
      "Epoch 103, Loss: 24272.9780\n",
      "Epoch 104, Loss: 24273.3415\n",
      "Epoch 105, Loss: 24276.0038\n",
      "Epoch 106, Loss: 24266.9702\n",
      "Epoch 107, Loss: 24254.6640\n",
      "Epoch 108, Loss: 24299.5161\n",
      "Epoch 109, Loss: 24262.2806\n",
      "Epoch 110, Loss: 24267.6972\n",
      "Epoch 111, Loss: 24236.9517\n",
      "Epoch 112, Loss: 24259.4622\n",
      "Epoch 113, Loss: 24252.6344\n",
      "Epoch 114, Loss: 24255.1899\n",
      "Epoch 115, Loss: 24229.0761\n",
      "Epoch 116, Loss: 24262.9496\n",
      "Epoch 117, Loss: 24240.2355\n",
      "Epoch 118, Loss: 24252.5661\n",
      "Epoch 119, Loss: 24242.8868\n",
      "Epoch 120, Loss: 24242.7322\n",
      "Epoch 121, Loss: 24255.4922\n",
      "Epoch 122, Loss: 24243.0548\n",
      "Epoch 123, Loss: 24237.7727\n",
      "Epoch 124, Loss: 24230.2595\n",
      "Epoch 125, Loss: 24237.9937\n",
      "Epoch 126, Loss: 24226.0984\n",
      "Epoch 127, Loss: 24225.2936\n",
      "Epoch 128, Loss: 24231.3195\n",
      "Epoch 129, Loss: 24230.1602\n",
      "Epoch 130, Loss: 24233.9809\n",
      "Epoch 131, Loss: 24214.8448\n",
      "Epoch 132, Loss: 24219.0025\n",
      "Epoch 133, Loss: 24225.0562\n",
      "Epoch 134, Loss: 24228.6264\n",
      "Epoch 135, Loss: 24227.6119\n",
      "Epoch 136, Loss: 24212.4444\n",
      "Epoch 137, Loss: 24218.4437\n",
      "Epoch 138, Loss: 24223.0540\n",
      "Epoch 139, Loss: 24209.3675\n",
      "Epoch 140, Loss: 24207.1068\n",
      "Epoch 141, Loss: 24210.7659\n",
      "Epoch 142, Loss: 24225.2342\n",
      "Epoch 143, Loss: 24204.0999\n",
      "Epoch 144, Loss: 24226.0750\n",
      "Epoch 145, Loss: 24204.3584\n",
      "Epoch 146, Loss: 24205.1512\n",
      "Epoch 147, Loss: 24212.9977\n",
      "Epoch 148, Loss: 24200.2173\n",
      "Epoch 149, Loss: 24193.5198\n",
      "Epoch 150, Loss: 24208.7805\n",
      "Epoch 151, Loss: 24206.3518\n",
      "Epoch 152, Loss: 24184.5502\n",
      "Epoch 153, Loss: 24189.8585\n",
      "Epoch 154, Loss: 24183.3879\n",
      "Epoch 155, Loss: 24177.7897\n",
      "Epoch 156, Loss: 24190.3505\n",
      "Epoch 157, Loss: 24185.9484\n",
      "Epoch 158, Loss: 24191.2153\n",
      "Epoch 159, Loss: 24183.2253\n",
      "Epoch 160, Loss: 24170.8680\n",
      "Epoch 161, Loss: 24173.7496\n",
      "Epoch 162, Loss: 24172.4411\n",
      "Epoch 163, Loss: 24160.3856\n",
      "Epoch 164, Loss: 24177.9654\n",
      "Epoch 165, Loss: 24148.2621\n",
      "Epoch 166, Loss: 24154.1364\n",
      "Epoch 167, Loss: 24165.6408\n",
      "Epoch 168, Loss: 24132.3890\n",
      "Epoch 169, Loss: 24148.2571\n",
      "Epoch 170, Loss: 24167.0734\n",
      "Epoch 171, Loss: 24133.4355\n",
      "Epoch 172, Loss: 24137.3515\n",
      "Epoch 173, Loss: 24133.4805\n",
      "Epoch 174, Loss: 24135.7437\n",
      "Epoch 175, Loss: 24127.7246\n",
      "Epoch 176, Loss: 24143.3449\n",
      "Epoch 177, Loss: 24140.3895\n",
      "Epoch 178, Loss: 24137.8378\n",
      "Epoch 179, Loss: 24143.8150\n",
      "Epoch 180, Loss: 24152.2219\n",
      "Epoch 181, Loss: 24128.8492\n",
      "Epoch 182, Loss: 24110.7677\n",
      "Epoch 183, Loss: 24127.6855\n",
      "Epoch 184, Loss: 24134.1407\n",
      "Epoch 185, Loss: 24107.3139\n",
      "Epoch 186, Loss: 24104.8525\n",
      "Epoch 187, Loss: 24117.4467\n",
      "Epoch 188, Loss: 24100.1929\n",
      "Epoch 189, Loss: 24080.8233\n",
      "Epoch 190, Loss: 24103.5738\n",
      "Epoch 191, Loss: 24094.0073\n",
      "Epoch 192, Loss: 24102.1488\n",
      "Epoch 193, Loss: 24114.3456\n",
      "Epoch 194, Loss: 24089.7096\n",
      "Epoch 195, Loss: 24102.3081\n",
      "Epoch 196, Loss: 24072.8859\n",
      "Epoch 197, Loss: 24092.0789\n",
      "Epoch 198, Loss: 24101.8556\n",
      "Epoch 199, Loss: 24101.3277\n",
      "Epoch 200, Loss: 24065.7445\n"
     ]
    }
   ],
   "source": [
    "model = ProdLDA(vocab_size=X_train_tensor.shape[1], num_topics=50).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 200\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(0, X_train_tensor.size(0), 64):\n",
    "        batch = X_train_tensor[i:i+64]\n",
    "        output = model(batch)\n",
    "        loss = output['loss']\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T14:11:51.224542Z",
     "iopub.status.busy": "2025-05-21T14:11:51.224307Z",
     "iopub.status.idle": "2025-05-21T14:11:51.309606Z",
     "shell.execute_reply": "2025-05-21T14:11:51.309069Z",
     "shell.execute_reply.started": "2025-05-21T14:11:51.224514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_theta(model, X_tensor):\n",
    "    model.eval()\n",
    "    theta_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, X_tensor.size(0), 64):\n",
    "            batch = X_tensor[i:i+64]\n",
    "            theta = model.get_theta(batch)\n",
    "            if isinstance(theta, tuple):\n",
    "                theta = theta[0]\n",
    "            theta_list.append(theta.cpu())\n",
    "    return torch.cat(theta_list).numpy()\n",
    "\n",
    "X_train_theta = extract_theta(model, X_train_tensor)\n",
    "X_test_theta = extract_theta(model, X_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T14:11:51.310526Z",
     "iopub.status.busy": "2025-05-21T14:11:51.310297Z",
     "iopub.status.idle": "2025-05-21T14:11:54.480871Z",
     "shell.execute_reply": "2025-05-21T14:11:54.480132Z",
     "shell.execute_reply.started": "2025-05-21T14:11:51.310509Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.8164\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85       607\n",
      "           1       0.92      0.92      0.92       627\n",
      "           2       0.75      0.72      0.74       626\n",
      "           3       0.73      0.80      0.76       640\n",
      "\n",
      "    accuracy                           0.82      2500\n",
      "   macro avg       0.82      0.82      0.82      2500\n",
      "weighted avg       0.82      0.82      0.82      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "clf = SVC(kernel='rbf', gamma='scale')\n",
    "clf.fit(X_train_theta, label_train)\n",
    "y_pred = clf.predict(X_test_theta)\n",
    "\n",
    "print(\"\\nAccuracy:\", accuracy_score(label_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(label_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T14:11:54.482048Z",
     "iopub.status.busy": "2025-05-21T14:11:54.481774Z",
     "iopub.status.idle": "2025-05-21T14:14:04.327487Z",
     "shell.execute_reply": "2025-05-21T14:14:04.326632Z",
     "shell.execute_reply.started": "2025-05-21T14:11:54.482026Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Naive Bayes...\n",
      "Saved Naive Bayes model\n",
      "Training SVM...\n",
      "Saved SVM model\n",
      "Training Logistic Regression...\n",
      "Saved Logistic Regression model\n",
      "Training SGD...\n",
      "Saved SGD model\n",
      "Training Random Forest...\n",
      "Saved Random Forest model\n",
      "Saved vectorizer\n",
      "\n",
      "All models have been saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "\n",
    "dataset_path = \"/kaggle/working/ECRTM/data/AGNews\"\n",
    "\n",
    "with open(f\"{dataset_path}/train_texts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_train = f.read().splitlines()\n",
    "\n",
    "with open(f\"{dataset_path}/train_labels.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    label_train = f.read().splitlines()\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10000)\n",
    "X_train_vec = vectorizer.fit_transform(text_train)\n",
    "y_train = list(map(int, label_train))\n",
    "\n",
    "print(\"Training Naive Bayes...\")\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_vec, y_train)\n",
    "with open('nb_model.pkl', 'wb') as f:\n",
    "    pickle.dump(nb_model, f)\n",
    "print(\"Saved Naive Bayes model\")\n",
    "\n",
    "print(\"Training SVM...\")\n",
    "svm_model = SVC(kernel='rbf', gamma='scale', probability=True)\n",
    "svm_model.fit(X_train_vec, y_train)\n",
    "with open('svm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_model, f)\n",
    "print(\"Saved SVM model\")\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "lr_model.fit(X_train_vec, y_train)\n",
    "with open('lr_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "print(\"Saved Logistic Regression model\")\n",
    "\n",
    "print(\"Training SGD...\")\n",
    "sgd_model = SGDClassifier(loss=\"log_loss\", max_iter=1000, random_state=0)\n",
    "sgd_model.fit(X_train_vec, y_train)\n",
    "with open('sgd_model.pkl', 'wb') as f:\n",
    "    pickle.dump(sgd_model, f)\n",
    "print(\"Saved SGD model\")\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train_vec, y_train)\n",
    "with open('rf_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "print(\"Saved Random Forest model\")\n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "print(\"Saved vectorizer\")\n",
    "\n",
    "print(\"\\nAll models have been saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T14:40:23.403610Z",
     "iopub.status.busy": "2025-05-21T14:40:23.402950Z",
     "iopub.status.idle": "2025-05-21T14:40:33.022781Z",
     "shell.execute_reply": "2025-05-21T14:40:33.022040Z",
     "shell.execute_reply.started": "2025-05-21T14:40:23.403584Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.30.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (22.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.10.1 (from gradio)\n",
      "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.11.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Downloading gradio-5.30.0-py3-none-any.whl (54.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.11.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Installing collected packages: uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, starlette, safehttpx, gradio-client, fastapi, gradio\n",
      "Successfully installed fastapi-0.115.12 ffmpy-0.5.0 gradio-5.30.0 gradio-client-1.10.1 groovy-0.1.2 python-multipart-0.0.20 ruff-0.11.10 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T15:30:09.872643Z",
     "iopub.status.busy": "2025-05-21T15:30:09.872372Z",
     "iopub.status.idle": "2025-05-21T15:30:10.920164Z",
     "shell.execute_reply": "2025-05-21T15:30:10.919651Z",
     "shell.execute_reply.started": "2025-05-21T15:30:09.872624Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "* Running on public URL: https://bb1e2929381b7d9e1e.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://bb1e2929381b7d9e1e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "import os\n",
    "\n",
    "vocab_path = \"/kaggle/working/ECRTM/data/AGNews/vocab.txt\"\n",
    "with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "    vocab = [line.strip() for line in f.readlines()]\n",
    "\n",
    "bow_vectorizer = CountVectorizer(vocabulary=dict(zip(vocab, range(len(vocab)))))\n",
    "\n",
    "label_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "checkpoint = torch.load('text_classifier.pth', weights_only=False)\n",
    "nn_vectorizer = checkpoint['vectorizer']\n",
    "\n",
    "input_dim = len(nn_vectorizer.get_feature_names_out())\n",
    "hidden_dim = 128\n",
    "output_dim = len(label_names)\n",
    "nn_model = SimpleNN(input_dim, hidden_dim, output_dim)\n",
    "nn_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "nn_model = nn_model.to(device)\n",
    "nn_model.eval()\n",
    "\n",
    "prodlda_model = ProdLDA(vocab_size=len(vocab), num_topics=50).to(device)\n",
    "prodlda_model.eval()\n",
    "\n",
    "model_dict = {\n",
    "    \"Naive Bayes\": nb_model,\n",
    "    \"Logistic Regression\": lr_model,\n",
    "    \"SVM\": svc_model,\n",
    "    \"SGD\": sgd_model,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"Neural Network\": nn_model,\n",
    "    \"Prod LDA\": prodlda_model,  \n",
    "    \"Classification (Kmeans)\": None\n",
    "}\n",
    "\n",
    "if model_dict[\"Classification (Kmeans)\"] is None:\n",
    "    X_train_vec = vectorizer.transform(text_train)\n",
    "    label_train_int = np.array(label_train, dtype=int)\n",
    "\n",
    "    centroids = []\n",
    "    for label in range(4):\n",
    "        group_indices = np.where(label_train_int == label)[0]\n",
    "        group_vectors = X_train_vec[group_indices]\n",
    "        centroid = group_vectors.mean(axis=0)\n",
    "        centroids.append(centroid)\n",
    "    centroids_matrix = np.asarray(np.vstack(centroids))\n",
    "    model_dict[\"Classification (Kmeans)\"] = centroids_matrix\n",
    "\n",
    "def classify_news(model_name, input_text):\n",
    "    if model_name == \"Neural Network\":\n",
    "        vec = nn_vectorizer.transform([input_text])\n",
    "        input_tensor = torch.FloatTensor(vec.toarray()).to(device)\n",
    "        model = model_dict[model_name]\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            pred = pred.item()\n",
    "    elif model_name == \"Prod LDA\":\n",
    "        bow = bow_vectorizer.transform([input_text])\n",
    "        input_tensor = torch.FloatTensor(bow.toarray()).to(device)\n",
    "        with torch.no_grad():\n",
    "            theta = model_dict[model_name].get_theta(input_tensor)\n",
    "            if isinstance(theta, tuple):\n",
    "                theta = theta[0]\n",
    "            pred = clf.predict(theta.cpu().numpy())[0]\n",
    "    elif model_name == \"Classification (Kmeans)\":\n",
    "        vec = vectorizer.transform([input_text])\n",
    "        similarities = cosine_similarity(vec, model_dict[\"Classification (Kmeans)\"])\n",
    "        pred = np.argmax(similarities, axis=1)[0]\n",
    "    else:\n",
    "        vec = vectorizer.transform([input_text])\n",
    "        model = model_dict[model_name]\n",
    "        pred = model.predict(vec)[0]\n",
    "\n",
    "    return label_names[int(pred)]\n",
    "\n",
    "gr.Interface(\n",
    "    fn=classify_news,\n",
    "    inputs=[\n",
    "        gr.Dropdown(choices=list(model_dict.keys()), label=\"Select Model\"),\n",
    "        gr.Textbox(lines=4, label=\"Enter News Content\")\n",
    "    ],\n",
    "    outputs=gr.Label(label=\"Predicted Category\"),\n",
    "    title=\"AG News Text Classification\",\n",
    "    description=\"Choose a model and enter a news article to classify it into one of 4 categories: World, Sports, Business, Sci/Tech.\"\n",
    ").launch()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7474257,
     "sourceId": 11891427,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
