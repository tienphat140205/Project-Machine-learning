{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"dataset_path = \"data\"\n\nwith open(f\"{dataset_path}/train_texts.txt\", \"r\", encoding=\"utf-8\") as f:\n    text_train = f.read().splitlines()\n\nwith open(f\"{dataset_path}/train_labels.txt\", \"r\", encoding=\"utf-8\") as f:\n    label_train = f.read().splitlines()\n\nwith open(f\"{dataset_path}/test_texts.txt\", \"r\", encoding=\"utf-8\") as f:\n    text_test = f.read().splitlines()\n\nwith open(f\"{dataset_path}/test_labels.txt\", \"r\", encoding=\"utf-8\") as f:\n    label_test = f.read().splitlines()\n\nprint(\"üìä S·ªë m·∫´u train:\", len(text_train))\nprint(\"üìä S·ªë nh√£n train:\", len(label_train))\nprint(\"üìä S·ªë m·∫´u test:\", len(text_test))\nprint(\"üìä S·ªë nh√£n test:\", len(label_test))\n\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-05-28T09:15:35.969081Z","iopub.execute_input":"2025-05-28T09:15:35.969368Z","iopub.status.idle":"2025-05-28T09:15:35.979884Z","shell.execute_reply.started":"2025-05-28T09:15:35.969347Z","shell.execute_reply":"2025-05-28T09:15:35.979262Z"},"trusted":true},"outputs":[{"name":"stdout","text":"üìä S·ªë m·∫´u train: 10000\nüìä S·ªë nh√£n train: 10000\nüìä S·ªë m·∫´u test: 2500\nüìä S·ªë nh√£n test: 2500\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"**Neural Network**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n\n# Check device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nvectorizer = TfidfVectorizer(stop_words='english', max_features=20000)\nX_train_vec = vectorizer.fit_transform(text_train)\nX_test_vec = vectorizer.transform(text_test)\n\nlabel_encoder = LabelEncoder()\ny_train_enc = label_encoder.fit_transform(label_train)\ny_test_enc = label_encoder.transform(label_test)\n\nX_train_tensor = torch.FloatTensor(X_train_vec.toarray())\ny_train_tensor = torch.LongTensor(y_train_enc)\nX_test_tensor = torch.FloatTensor(X_test_vec.toarray())\ny_test_tensor = torch.LongTensor(y_test_enc)\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64)\n\nclass SimpleNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        self.relu3 = nn.ReLU()\n        self.output = nn.Linear(hidden_dim, output_dim)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu1(x)\n        x = self.fc2(x)\n        x = self.relu2(x)\n        x = self.fc3(x)\n        x = self.relu3(x)\n        x = self.output(x)\n        return x\n\ninput_dim = X_train_vec.shape[1]\nhidden_dim = 128\noutput_dim = len(label_encoder.classes_)\nmodel = SimpleNN(input_dim, hidden_dim, output_dim).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 2\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f\"üìà Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# --- Metrics ---\naccuracy = accuracy_score(all_labels, all_preds)\nprecision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n\nprint(\"\\n‚úÖ ƒê√°nh gi√° tr√™n t·∫≠p test:\")\nprint(f\"üéØ Accuracy :  {accuracy:.4f}\")\nprint(f\"üéØ Precision:  {precision:.4f}\")\nprint(f\"üéØ Recall   :  {recall:.4f}\")\nprint(f\"üéØ F1-score :  {f1:.4f}\")\nprint(\"\\nüìã Classification Report:\")\nprint(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'vectorizer': vectorizer,\n    'label_encoder': label_encoder\n}, 'nn_model.pth')\nprint(\"\\nüíæ ƒê√£ l∆∞u m√¥ h√¨nh v√†o text_classifier.pth\")\n\ndef predict(text):\n    model.eval()\n    vec = vectorizer.transform([text])\n    input_tensor = torch.FloatTensor(vec.toarray()).to(device)\n    with torch.no_grad():\n        outputs = model(input_tensor)\n        _, pred = torch.max(outputs, 1)\n    return label_encoder.inverse_transform(pred.cpu().numpy())[0]\n\nexample_text = \"How does inflation affect small businesses?\"\npredicted_label = predict(example_text)\nprint(f\"\\nüìù Input: {example_text}\")\nprint(f\"üîç Predicted topic: {predicted_label}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-05-28T09:28:27.230881Z","iopub.execute_input":"2025-05-28T09:28:27.231152Z","iopub.status.idle":"2025-05-28T09:28:29.039014Z","shell.execute_reply.started":"2025-05-28T09:28:27.231135Z","shell.execute_reply":"2025-05-28T09:28:29.038401Z"},"trusted":true},"outputs":[{"name":"stdout","text":"üìà Epoch 1/2, Loss: 0.7175\nüìà Epoch 2/2, Loss: 0.1954\n\n‚úÖ ƒê√°nh gi√° tr√™n t·∫≠p test:\nüéØ Accuracy :  0.8824\nüéØ Precision:  0.8827\nüéØ Recall   :  0.8824\nüéØ F1-score :  0.8822\n\nüìã Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.92      0.86      0.88       607\n           1       0.93      0.97      0.95       627\n           2       0.83      0.85      0.84       626\n           3       0.86      0.85      0.85       640\n\n    accuracy                           0.88      2500\n   macro avg       0.88      0.88      0.88      2500\nweighted avg       0.88      0.88      0.88      2500\n\n\nüíæ ƒê√£ l∆∞u m√¥ h√¨nh v√†o text_classifier.pth\n\nüìù Input: How does inflation affect small businesses?\nüîç Predicted topic: 2\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"**Naive Bayes**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import (\n    classification_report, accuracy_score,\n    precision_score, recall_score, f1_score\n)\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\ny_train_enc = label_encoder.fit_transform(label_train)\ny_test_enc = label_encoder.transform(label_test)\n\nvectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10000)\nX_train_vec = vectorizer.fit_transform(text_train)\nX_test_vec = vectorizer.transform(text_test)\n\nnb_model = MultinomialNB()\nnb_model.fit(X_train_vec, y_train_enc)\n\ny_pred = nb_model.predict(X_test_vec)\n\nlabel_names = label_encoder.classes_\n\nprint(\"\\n‚úÖ ƒê√°nh gi√° m√¥ h√¨nh Naive Bayes:\")\nprint(f\"üîπ Accuracy       : {accuracy_score(y_test_enc, y_pred):.4f}\")\nprint(f\"üîπ Precision (weighted): {precision_score(y_test_enc, y_pred, average='weighted'):.4f}\")\nprint(f\"üîπ Recall    (weighted): {recall_score(y_test_enc, y_pred, average='weighted'):.4f}\")\nprint(f\"üîπ F1-score  (weighted): {f1_score(y_test_enc, y_pred, average='weighted'):.4f}\")\nprint(f\"üîπ F1-score    (macro): {f1_score(y_test_enc, y_pred, average='macro'):.4f}\")\n\nprint(\"\\nüìã Classification Report:\\n\")\nprint(classification_report(y_test_enc, y_pred, target_names=label_names))\n\ndef predict_topic(text):\n    vec = vectorizer.transform([text])\n    pred = nb_model.predict(vec)[0]\n    return label_encoder.inverse_transform([pred])[0]\nimport joblib\njoblib.dump(nb_model, \"nb_model.pkl\")\njoblib.dump(vectorizer, \"nb_vectorizer.pkl\")\njoblib.dump(label_encoder, \"nb_label_encoder.pkl\")\nexample = \"How does inflation affect small businesses?\"\nprint(\"\\nüìù Input:\", example)\nprint(\"üîç Predicted topic:\", predict_topic(example))\n","metadata":{"execution":{"iopub.status.busy":"2025-05-28T09:28:46.620897Z","iopub.execute_input":"2025-05-28T09:28:46.621208Z","iopub.status.idle":"2025-05-28T09:28:46.904829Z","shell.execute_reply.started":"2025-05-28T09:28:46.621182Z","shell.execute_reply":"2025-05-28T09:28:46.904189Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n‚úÖ ƒê√°nh gi√° m√¥ h√¨nh Naive Bayes:\nüîπ Accuracy       : 0.8788\nüîπ Precision (weighted): 0.8784\nüîπ Recall    (weighted): 0.8788\nüîπ F1-score  (weighted): 0.8786\nüîπ F1-score    (macro): 0.8788\n\nüìã Classification Report:\n\n              precision    recall  f1-score   support\n\n           0       0.89      0.88      0.89       607\n           1       0.94      0.96      0.95       627\n           2       0.83      0.83      0.83       626\n           3       0.85      0.85      0.85       640\n\n    accuracy                           0.88      2500\n   macro avg       0.88      0.88      0.88      2500\nweighted avg       0.88      0.88      0.88      2500\n\n\nüìù Input: How does inflation affect small businesses?\nüîç Predicted topic: 2\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"**Logistic Regression**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\nvectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10000)\n\nX_train_vec = vectorizer.fit_transform(text_train)\nX_test_vec = vectorizer.transform(text_test)\n\ny_train = list(map(int, label_train))\ny_test = list(map(int, label_test))\n\nlr_model = LogisticRegression(max_iter=1000, solver='lbfgs') \nlr_model.fit(X_train_vec, y_train)\n\ny_pred = lr_model.predict(X_test_vec)\n\nlabel_names = [f\"Topic {i}\" for i in sorted(set(y_train))]\n\nprint(\"‚úÖ Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"\\n‚úÖ Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=label_names))\n\ndef predict_topic(text):\n    vec = vectorizer.transform([text])\n    pred = lr_model.predict(vec)[0]\n    return label_names[int(pred)]\njoblib.dump(lr_model, \"lr_model.pkl\")\njoblib.dump(vectorizer, \"lr_vectorizer.pkl\")\njoblib.dump(label_encoder, \"lr_label_encoder.pkl\")\nexample = \"How does inflation affect small businesses?\"\nprint(\"\\nüìå Predicted topic:\", predict_topic(example))\n","metadata":{"execution":{"iopub.status.busy":"2025-05-28T09:28:55.030672Z","iopub.execute_input":"2025-05-28T09:28:55.031344Z","iopub.status.idle":"2025-05-28T09:28:56.488617Z","shell.execute_reply.started":"2025-05-28T09:28:55.031323Z","shell.execute_reply":"2025-05-28T09:28:56.488016Z"},"trusted":true},"outputs":[{"name":"stdout","text":"‚úÖ Accuracy: 0.8828\n\n‚úÖ Classification Report:\n              precision    recall  f1-score   support\n\n     Topic 0       0.88      0.89      0.89       607\n     Topic 1       0.93      0.96      0.95       627\n     Topic 2       0.86      0.83      0.85       626\n     Topic 3       0.85      0.85      0.85       640\n\n    accuracy                           0.88      2500\n   macro avg       0.88      0.88      0.88      2500\nweighted avg       0.88      0.88      0.88      2500\n\n\nüìå Predicted topic: Topic 3\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"**SGD**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport numpy as np\n\n\nvectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10000)\nX_train_vec = vectorizer.fit_transform(text_train)\nX_test_vec = vectorizer.transform(text_test)\n\ny_train = list(map(int, label_train))\ny_test = list(map(int, label_test))\n\nsgd_model = SGDClassifier(\n    loss=\"log_loss\",   \n    random_state=0,\n    n_jobs=-1           \n)\nsgd_model.fit(X_train_vec, y_train)\n\ny_pred = sgd_model.predict(X_test_vec)\n\n\n\nprint(\"‚úÖ Accuracy (SGD):\", accuracy_score(y_test, y_pred))\nprint(\"\\nüìã Classification Report:\\n\")\nprint(classification_report(y_test, y_pred))\njoblib.dump(sgd_model, \"sgd_model.pkl\")\njoblib.dump(vectorizer, \"sgd_vectorizer.pkl\")\njoblib.dump(label_encoder, \"sgd_label_encoder.pkl\")","metadata":{"execution":{"iopub.status.busy":"2025-05-28T09:29:06.378983Z","iopub.execute_input":"2025-05-28T09:29:06.379513Z","iopub.status.idle":"2025-05-28T09:29:06.682558Z","shell.execute_reply.started":"2025-05-28T09:29:06.379494Z","shell.execute_reply":"2025-05-28T09:29:06.682020Z"},"trusted":true},"outputs":[{"name":"stdout","text":"‚úÖ Accuracy (SGD): 0.8828\n\nüìã Classification Report:\n\n              precision    recall  f1-score   support\n\n           0       0.88      0.88      0.88       607\n           1       0.93      0.96      0.95       627\n           2       0.87      0.83      0.85       626\n           3       0.85      0.86      0.85       640\n\n    accuracy                           0.88      2500\n   macro avg       0.88      0.88      0.88      2500\nweighted avg       0.88      0.88      0.88      2500\n\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"['sgd_label_encoder.pkl']"},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"**SVM**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\nvectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10000)\nX_train_vec = vectorizer.fit_transform(text_train)\nX_test_vec = vectorizer.transform(text_test)\n\ny_train = list(map(int, label_train))\ny_test = list(map(int, label_test))\n\nsvc_model = SVC(kernel='rbf', gamma='scale') \nsvc_model.fit(X_train_vec, y_train)\n\ny_pred = svc_model.predict(X_test_vec)\n\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\njoblib.dump(svc_model, \"svc_model.pkl\")\njoblib.dump(vectorizer, \"svc_vectorizer.pkl\")\njoblib.dump(label_encoder, \"svc_label_encoder.pkl\")","metadata":{"execution":{"iopub.status.busy":"2025-05-28T09:29:17.376651Z","iopub.execute_input":"2025-05-28T09:29:17.377250Z","iopub.status.idle":"2025-05-28T09:29:43.361930Z","shell.execute_reply.started":"2025-05-28T09:29:17.377227Z","shell.execute_reply":"2025-05-28T09:29:43.361367Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Accuracy: 0.8908\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.89      0.88      0.89       607\n           1       0.94      0.96      0.95       627\n           2       0.88      0.84      0.86       626\n           3       0.85      0.87      0.86       640\n\n    accuracy                           0.89      2500\n   macro avg       0.89      0.89      0.89      2500\nweighted avg       0.89      0.89      0.89      2500\n\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"['svc_label_encoder.pkl']"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"**Random Forest**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport numpy as np\n\n\n\nvectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10000)\nX_train_vec = vectorizer.fit_transform(text_train)\nX_test_vec = vectorizer.transform(text_test)\n\ny_train = list(map(int, label_train))\ny_test = list(map(int, label_test))\n\nrf_model = RandomForestClassifier(\n    n_estimators=200,        \n    max_depth=1500,          \n    min_samples_split=2,     \n    random_state=42,\n    n_jobs=-1                \n)\nrf_model.fit(X_train_vec, y_train)\n\ny_pred = rf_model.predict(X_test_vec)\n\nprint(\"‚úÖ Accuracy (Random Forest):\", accuracy_score(y_test, y_pred))\nprint(\"\\nüìã Classification Report:\\n\")\nprint(classification_report(y_test, y_pred, target_names=label_names))\njoblib.dump(rf_model, \"rf_model.pkl\")\njoblib.dump(vectorizer, \"rf_vectorizer.pkl\")\njoblib.dump(label_encoder, \"rf_label_encoder.pkl\")","metadata":{"execution":{"iopub.status.busy":"2025-05-28T09:31:19.585395Z","iopub.execute_input":"2025-05-28T09:31:19.585983Z","iopub.status.idle":"2025-05-28T09:31:25.533070Z","shell.execute_reply.started":"2025-05-28T09:31:19.585959Z","shell.execute_reply":"2025-05-28T09:31:25.532498Z"},"trusted":true},"outputs":[{"name":"stdout","text":"‚úÖ Accuracy (Random Forest): 0.8392\n\nüìã Classification Report:\n\n              precision    recall  f1-score   support\n\n     Topic 0       0.85      0.85      0.85       607\n     Topic 1       0.90      0.93      0.91       627\n     Topic 2       0.82      0.78      0.80       626\n     Topic 3       0.79      0.80      0.80       640\n\n    accuracy                           0.84      2500\n   macro avg       0.84      0.84      0.84      2500\nweighted avg       0.84      0.84      0.84      2500\n\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"['rf_label_encoder.pkl']"},"metadata":{}}],"execution_count":35},{"cell_type":"markdown","source":"**Clustering**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport numpy as np\nimport joblib\nvectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10000)\nX_train_vec = vectorizer.fit_transform(text_train)\nX_test_vec = vectorizer.transform(text_test)\n\nlabel_train_int = np.array(label_train, dtype=int)\nlabel_test_int = np.array(label_test, dtype=int)\navailable_labels = np.unique(label_train_int)\n\ncentroids = []\nfor label in available_labels:\n    group_indices = np.where(label_train_int == label)[0]\n    group_vectors = X_train_vec[group_indices]\n    centroid = group_vectors.mean(axis=0)\n    centroids.append(centroid)\n\ncentroids_matrix = np.asarray(np.vstack(centroids))\nsimilarities = cosine_similarity(X_test_vec, centroids_matrix)\ny_pred = available_labels[np.argmax(similarities, axis=1)]\n\n\nprint(\"‚úÖ Accuracy:\", accuracy_score(label_test_int, y_pred))\nprint(\"\\nüìÑ Classification Report:\\n\", classification_report(label_test_int, y_pred, digits=4))\n# Save vectorizer and centroids\njoblib.dump(vectorizer, \"cluster_vectorizer.pkl\")\njoblib.dump(centroids_matrix, \"cluster_centroids.pkl\")\n","metadata":{"execution":{"iopub.status.busy":"2025-05-28T10:49:54.842991Z","iopub.execute_input":"2025-05-28T10:49:54.843428Z","iopub.status.idle":"2025-05-28T10:49:55.188868Z","shell.execute_reply.started":"2025-05-28T10:49:54.843399Z","shell.execute_reply":"2025-05-28T10:49:55.188350Z"},"trusted":true},"outputs":[{"name":"stdout","text":"‚úÖ Accuracy: 0.8552\n\nüìÑ Classification Report:\n               precision    recall  f1-score   support\n\n           0     0.8553    0.8764    0.8657       607\n           1     0.9244    0.9362    0.9303       627\n           2     0.8294    0.7843    0.8062       626\n           3     0.8111    0.8250    0.8180       640\n\n    accuracy                         0.8552      2500\n   macro avg     0.8550    0.8555    0.8551      2500\nweighted avg     0.8548    0.8552    0.8548      2500\n\n\n‚úÖ Vectorizer and centroids have been saved successfully!\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"**PROLDA**","metadata":{}},{"cell_type":"code","source":"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\nclass ProdLDA(nn.Module):\n    '''\n        Autoencoding Variational Inference For Topic Models. ICLR 2017\n\n        Akash Srivastava, Charles Sutton.\n    '''\n    def __init__(self, vocab_size, num_topics=50, en_units=200, dropout=0.4):\n        super().__init__()\n\n        self.num_topics = num_topics\n\n        self.a = 1 * np.ones((1, num_topics)).astype(np.float32)\n        self.mu2 = nn.Parameter(torch.as_tensor((np.log(self.a).T - np.mean(np.log(self.a), 1)).T))\n        self.var2 = nn.Parameter(torch.as_tensor((((1.0 / self.a) * (1 - (2.0 / num_topics))).T + (1.0 / (num_topics * num_topics)) * np.sum(1.0 / self.a, 1)).T))\n\n        self.mu2.requires_grad = False\n        self.var2.requires_grad = False\n\n        self.fc11 = nn.Linear(vocab_size, en_units)\n        self.fc12 = nn.Linear(en_units, en_units)\n        self.fc21 = nn.Linear(en_units, num_topics)\n        self.fc22 = nn.Linear(en_units, num_topics)\n\n        self.mean_bn = nn.BatchNorm1d(num_topics, eps=0.001, momentum=0.001, affine=True)\n        self.mean_bn.weight.data.copy_(torch.ones(num_topics))\n        self.mean_bn.weight.requires_grad = False\n\n        self.logvar_bn = nn.BatchNorm1d(num_topics, eps=0.001, momentum=0.001, affine=True)\n        self.logvar_bn.weight.data.copy_(torch.ones(num_topics))\n        self.logvar_bn.weight.requires_grad = False\n\n        self.decoder_bn = nn.BatchNorm1d(vocab_size, eps=0.001, momentum=0.001, affine=True)\n        self.decoder_bn.weight.data.copy_(torch.ones(vocab_size))\n        self.decoder_bn.weight.requires_grad = False\n\n        self.fc1_drop = nn.Dropout(dropout)\n        self.theta_drop = nn.Dropout(dropout)\n\n        self.fcd1 = nn.Linear(num_topics, vocab_size, bias=False)\n        nn.init.xavier_uniform_(self.fcd1.weight)\n\n    def get_beta(self):\n        return self.fcd1.weight.T\n\n    def get_theta(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        theta = F.softmax(z, dim=1)\n        theta = self.theta_drop(theta)\n        if self.training:\n            return theta, mu, logvar\n        else:\n            return theta\n\n    def reparameterize(self, mu, logvar):\n        if self.training:\n            std = torch.exp(0.5 * logvar)\n            eps = torch.randn_like(std)\n            return mu + (eps * std)\n        else:\n            return mu\n\n    def encode(self, x):\n        e1 = F.softplus(self.fc11(x))\n        e1 = F.softplus(self.fc12(e1))\n        e1 = self.fc1_drop(e1)\n        return self.mean_bn(self.fc21(e1)), self.logvar_bn(self.fc22(e1))\n\n    def decode(self, theta):\n        d1 = F.softmax(self.decoder_bn(self.fcd1(theta)), dim=1)\n        return d1\n\n    def forward(self, x):\n        theta, mu, logvar = self.get_theta(x)\n        recon_x = self.decode(theta)\n        loss = self.loss_function(x, recon_x, mu, logvar)\n        return {'loss': loss}\n\n    def loss_function(self, x, recon_x, mu, logvar):\n        recon_loss = -(x * (recon_x + 1e-10).log()).sum(axis=1)\n        var = logvar.exp()\n        var_division = var / self.var2\n        diff = mu - self.mu2\n        diff_term = diff * diff / self.var2\n        logvar_division = self.var2.log() - logvar\n        KLD = 0.5 * ((var_division + diff_term + logvar_division).sum(axis=1) - self.num_topics)\n        loss = (recon_loss + KLD).mean()\n        return loss","metadata":{"execution":{"iopub.status.busy":"2025-05-28T09:54:25.737221Z","iopub.execute_input":"2025-05-28T09:54:25.737707Z","iopub.status.idle":"2025-05-28T09:54:25.750496Z","shell.execute_reply.started":"2025-05-28T09:54:25.737684Z","shell.execute_reply":"2025-05-28T09:54:25.749682Z"},"trusted":true},"outputs":[],"execution_count":45},{"cell_type":"code","source":"from scipy import sparse\nimport os\nimport torch\nimport numpy as np\nimport joblib\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, accuracy_score\n# Load vocabulary\ndef load_vocab(vocab_path):\n    with open(vocab_path, 'r', encoding='utf-8') as f:\n        vocab = [line.strip() for line in f.readlines()]\n    return {word: idx for idx, word in enumerate(vocab)}\n\n# Load texts\ndef load_texts(text_path):\n    with open(text_path, 'r', encoding='utf-8') as f:\n        texts = [line.strip() for line in f.readlines()]\n    return texts\n\n# Set dataset path\ndataset_path = \"/kaggle/working/ECRTM/data/AGNews\"\n\n# Load vocabulary v√† texts\nvocab_path = os.path.join(dataset_path, \"vocab.txt\")\nvocab_dict = load_vocab(vocab_path)\n\ntrain_texts_path = os.path.join(dataset_path, \"train_texts.txt\")\ntest_texts_path = os.path.join(dataset_path, \"test_texts.txt\")\n\ntrain_texts = load_texts(train_texts_path)\ntest_texts = load_texts(test_texts_path)\n\n# T·∫°o BOW b·∫±ng sklearn\nprint(\"Creating BOW matrices with sklearn...\")\nvectorizer = CountVectorizer(vocabulary=vocab_dict, lowercase=True, stop_words='english')\nX_train_sparse = vectorizer.fit_transform(train_texts)\nX_test_sparse = vectorizer.transform(test_texts)\n\n\n# Convert to tensors\nX_train_tensor = torch.FloatTensor(X_train_sparse.toarray())\nX_test_tensor = torch.FloatTensor(X_test_sparse.toarray())\n\n# Move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nX_train_tensor = X_train_tensor.to(device)\nX_test_tensor = X_test_tensor.to(device)\n\n# Initialize model\nmodel = ProdLDA(vocab_size=X_train_tensor.shape[1], num_topics=50).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# Training\nepochs = 500\nmodel.train()\nfor epoch in range(epochs):\n    total_loss = 0\n    for i in range(0, X_train_tensor.size(0), 64):\n        batch = X_train_tensor[i:i+64]\n        output = model(batch)\n        loss = output['loss']\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n\n# Extract theta\ndef extract_theta(model, X_tensor):\n    model.eval()\n    theta_list = []\n    with torch.no_grad():\n        for i in range(0, X_tensor.size(0), 64):\n            batch = X_tensor[i:i+64]\n            theta = model.get_theta(batch)\n            if isinstance(theta, tuple):\n                theta = theta[0]\n            theta_list.append(theta.cpu())\n    return torch.cat(theta_list).numpy()\n\nX_train_theta = extract_theta(model, X_train_tensor)# 1. L∆∞u ProdLDA model weights\ntorch.save(model.state_dict(), \"prodlda.pth\")\n\n# 2. L∆∞u SVM classifier\njoblib.dump(clf, \"prodlda_svm.pkl\")\n\n# 3. L∆∞u CountVectorizer\njoblib.dump(vectorizer, \"prodlda_bow_vectorizer.pkl\")\nX_test_theta = extract_theta(model, X_test_tensor)\n\n\nclf = SVC(kernel='rbf', gamma='scale')\nclf.fit(X_train_theta, label_train)\ny_pred = clf.predict(X_test_theta)\n\nprint(\"\\nAccuracy:\", accuracy_score(label_test, y_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(label_test, y_pred))\n# 1. L∆∞u ProdLDA model weights\ntorch.save(model.state_dict(), \"prodlda.pth\")\n\n# 2. L∆∞u SVM classifier\njoblib.dump(clf, \"prodlda_svm.pkl\")\n\n# 3. L∆∞u CountVectorizer\njoblib.dump(vectorizer, \"prodlda_bow_vectorizer.pkl\")","metadata":{"execution":{"iopub.status.busy":"2025-05-28T09:54:26.300563Z","iopub.execute_input":"2025-05-28T09:54:26.301036Z","iopub.status.idle":"2025-05-28T09:58:28.763899Z","shell.execute_reply.started":"2025-05-28T09:54:26.301014Z","shell.execute_reply":"2025-05-28T09:58:28.763172Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Creating BOW matrices with sklearn...\nEpoch 1, Loss: 28851.8849\nEpoch 2, Loss: 24750.3757\nEpoch 3, Loss: 24566.9622\nEpoch 4, Loss: 24401.4527\nEpoch 5, Loss: 24253.1013\nEpoch 6, Loss: 24109.1375\nEpoch 7, Loss: 23986.9417\nEpoch 8, Loss: 23878.4209\nEpoch 9, Loss: 23774.5526\nEpoch 10, Loss: 23675.8763\nEpoch 11, Loss: 23592.5010\nEpoch 12, Loss: 23521.2204\nEpoch 13, Loss: 23450.5224\nEpoch 14, Loss: 23385.7997\nEpoch 15, Loss: 23329.9842\nEpoch 16, Loss: 23281.7941\nEpoch 17, Loss: 23231.9182\nEpoch 18, Loss: 23196.4272\nEpoch 19, Loss: 23150.4696\nEpoch 20, Loss: 23126.2598\nEpoch 21, Loss: 23088.6696\nEpoch 22, Loss: 23063.9016\nEpoch 23, Loss: 23037.3634\nEpoch 24, Loss: 23019.5582\nEpoch 25, Loss: 23001.4425\nEpoch 26, Loss: 22988.4018\nEpoch 27, Loss: 22974.8255\nEpoch 28, Loss: 22952.2537\nEpoch 29, Loss: 22958.6703\nEpoch 30, Loss: 22946.7143\nEpoch 31, Loss: 22931.9573\nEpoch 32, Loss: 22921.7793\nEpoch 33, Loss: 22920.2880\nEpoch 34, Loss: 22908.8436\nEpoch 35, Loss: 22904.4244\nEpoch 36, Loss: 22878.6336\nEpoch 37, Loss: 22874.8640\nEpoch 38, Loss: 22870.9964\nEpoch 39, Loss: 22856.6398\nEpoch 40, Loss: 22845.4804\nEpoch 41, Loss: 22827.9987\nEpoch 42, Loss: 22838.6095\nEpoch 43, Loss: 22825.4085\nEpoch 44, Loss: 22811.4010\nEpoch 45, Loss: 22818.4662\nEpoch 46, Loss: 22820.7542\nEpoch 47, Loss: 22806.3822\nEpoch 48, Loss: 22800.1318\nEpoch 49, Loss: 22791.8483\nEpoch 50, Loss: 22782.3701\nEpoch 51, Loss: 22789.9108\nEpoch 52, Loss: 22773.1826\nEpoch 53, Loss: 22781.0941\nEpoch 54, Loss: 22772.3054\nEpoch 55, Loss: 22775.1551\nEpoch 56, Loss: 22758.9714\nEpoch 57, Loss: 22762.2668\nEpoch 58, Loss: 22771.0354\nEpoch 59, Loss: 22762.8284\nEpoch 60, Loss: 22760.0748\nEpoch 61, Loss: 22750.4868\nEpoch 62, Loss: 22772.3211\nEpoch 63, Loss: 22732.2726\nEpoch 64, Loss: 22738.1105\nEpoch 65, Loss: 22757.0449\nEpoch 66, Loss: 22745.2717\nEpoch 67, Loss: 22753.2006\nEpoch 68, Loss: 22745.8473\nEpoch 69, Loss: 22733.6805\nEpoch 70, Loss: 22715.5514\nEpoch 71, Loss: 22736.1826\nEpoch 72, Loss: 22736.9931\nEpoch 73, Loss: 22722.3287\nEpoch 74, Loss: 22713.7004\nEpoch 75, Loss: 22715.1885\nEpoch 76, Loss: 22716.0542\nEpoch 77, Loss: 22716.2230\nEpoch 78, Loss: 22719.2275\nEpoch 79, Loss: 22701.8400\nEpoch 80, Loss: 22715.9891\nEpoch 81, Loss: 22688.8729\nEpoch 82, Loss: 22704.6861\nEpoch 83, Loss: 22705.2093\nEpoch 84, Loss: 22697.3664\nEpoch 85, Loss: 22702.1861\nEpoch 86, Loss: 22703.4675\nEpoch 87, Loss: 22700.0739\nEpoch 88, Loss: 22685.4702\nEpoch 89, Loss: 22696.5910\nEpoch 90, Loss: 22693.5718\nEpoch 91, Loss: 22684.5065\nEpoch 92, Loss: 22678.8284\nEpoch 93, Loss: 22688.3702\nEpoch 94, Loss: 22697.1400\nEpoch 95, Loss: 22667.8486\nEpoch 96, Loss: 22679.8884\nEpoch 97, Loss: 22680.4622\nEpoch 98, Loss: 22680.9781\nEpoch 99, Loss: 22680.5171\nEpoch 100, Loss: 22689.1688\nEpoch 101, Loss: 22778.6191\nEpoch 102, Loss: 22686.5116\nEpoch 103, Loss: 22667.2249\nEpoch 104, Loss: 22681.1864\nEpoch 105, Loss: 22662.3873\nEpoch 106, Loss: 22671.7832\nEpoch 107, Loss: 22672.8499\nEpoch 108, Loss: 22672.1602\nEpoch 109, Loss: 22676.7463\nEpoch 110, Loss: 22665.8958\nEpoch 111, Loss: 22669.4742\nEpoch 112, Loss: 22668.9016\nEpoch 113, Loss: 22678.5851\nEpoch 114, Loss: 22652.9060\nEpoch 115, Loss: 22660.5012\nEpoch 116, Loss: 22686.3975\nEpoch 117, Loss: 22643.8224\nEpoch 118, Loss: 22662.3376\nEpoch 119, Loss: 22651.9169\nEpoch 120, Loss: 22655.7648\nEpoch 121, Loss: 22653.4698\nEpoch 122, Loss: 22644.4755\nEpoch 123, Loss: 22658.8824\nEpoch 124, Loss: 22655.0686\nEpoch 125, Loss: 22631.9550\nEpoch 126, Loss: 22645.3970\nEpoch 127, Loss: 22656.4140\nEpoch 128, Loss: 22658.3206\nEpoch 129, Loss: 22668.1271\nEpoch 130, Loss: 22663.5969\nEpoch 131, Loss: 22645.5164\nEpoch 132, Loss: 22642.4120\nEpoch 133, Loss: 22643.1039\nEpoch 134, Loss: 22633.2132\nEpoch 135, Loss: 22624.6318\nEpoch 136, Loss: 22650.2979\nEpoch 137, Loss: 22646.7195\nEpoch 138, Loss: 22639.2724\nEpoch 139, Loss: 22637.7763\nEpoch 140, Loss: 22646.7019\nEpoch 141, Loss: 22634.6185\nEpoch 142, Loss: 22629.1119\nEpoch 143, Loss: 22641.1830\nEpoch 144, Loss: 22616.0906\nEpoch 145, Loss: 22693.2582\nEpoch 146, Loss: 22622.1603\nEpoch 147, Loss: 22633.5009\nEpoch 148, Loss: 22631.8185\nEpoch 149, Loss: 22637.0991\nEpoch 150, Loss: 22632.2236\nEpoch 151, Loss: 22625.2409\nEpoch 152, Loss: 22636.3072\nEpoch 153, Loss: 22798.9287\nEpoch 154, Loss: 22632.3614\nEpoch 155, Loss: 22635.7407\nEpoch 156, Loss: 22608.7321\nEpoch 157, Loss: 22615.1793\nEpoch 158, Loss: 22614.9035\nEpoch 159, Loss: 22606.1293\nEpoch 160, Loss: 22613.0677\nEpoch 161, Loss: 22607.7709\nEpoch 162, Loss: 22609.1319\nEpoch 163, Loss: 22633.1783\nEpoch 164, Loss: 22618.5796\nEpoch 165, Loss: 22599.5748\nEpoch 166, Loss: 22615.6499\nEpoch 167, Loss: 22619.6880\nEpoch 168, Loss: 22610.7917\nEpoch 169, Loss: 22580.4810\nEpoch 170, Loss: 22610.8165\nEpoch 171, Loss: 22597.6441\nEpoch 172, Loss: 22599.2921\nEpoch 173, Loss: 22606.8055\nEpoch 174, Loss: 22598.0108\nEpoch 175, Loss: 22596.2616\nEpoch 176, Loss: 22595.2044\nEpoch 177, Loss: 22593.4917\nEpoch 178, Loss: 22606.7251\nEpoch 179, Loss: 22593.5278\nEpoch 180, Loss: 22583.7928\nEpoch 181, Loss: 22584.9824\nEpoch 182, Loss: 22594.8591\nEpoch 183, Loss: 22588.3756\nEpoch 184, Loss: 22599.4114\nEpoch 185, Loss: 22569.2463\nEpoch 186, Loss: 22584.9507\nEpoch 187, Loss: 22593.2938\nEpoch 188, Loss: 22577.1681\nEpoch 189, Loss: 22587.6984\nEpoch 190, Loss: 22582.0609\nEpoch 191, Loss: 22589.8325\nEpoch 192, Loss: 22571.4531\nEpoch 193, Loss: 22577.7052\nEpoch 194, Loss: 22563.3159\nEpoch 195, Loss: 22578.3136\nEpoch 196, Loss: 22565.7032\nEpoch 197, Loss: 22591.9404\nEpoch 198, Loss: 22587.6247\nEpoch 199, Loss: 22572.6478\nEpoch 200, Loss: 22572.6406\nEpoch 201, Loss: 22556.7705\nEpoch 202, Loss: 22573.7295\nEpoch 203, Loss: 22556.1364\nEpoch 204, Loss: 22555.9698\nEpoch 205, Loss: 22574.6481\nEpoch 206, Loss: 22562.9558\nEpoch 207, Loss: 22554.9641\nEpoch 208, Loss: 22565.8858\nEpoch 209, Loss: 22550.7515\nEpoch 210, Loss: 22558.5343\nEpoch 211, Loss: 22546.4668\nEpoch 212, Loss: 22543.3878\nEpoch 213, Loss: 22554.3059\nEpoch 214, Loss: 22543.9743\nEpoch 215, Loss: 22560.0164\nEpoch 216, Loss: 22540.5669\nEpoch 217, Loss: 22531.9918\nEpoch 218, Loss: 22539.2390\nEpoch 219, Loss: 22555.4267\nEpoch 220, Loss: 22528.7716\nEpoch 221, Loss: 22544.9070\nEpoch 222, Loss: 22550.9632\nEpoch 223, Loss: 22545.4993\nEpoch 224, Loss: 22536.0839\nEpoch 225, Loss: 22526.4383\nEpoch 226, Loss: 22536.1535\nEpoch 227, Loss: 22519.3600\nEpoch 228, Loss: 22529.4239\nEpoch 229, Loss: 22521.2512\nEpoch 230, Loss: 22516.3051\nEpoch 231, Loss: 22518.7584\nEpoch 232, Loss: 22522.6324\nEpoch 233, Loss: 22524.1717\nEpoch 234, Loss: 22513.5348\nEpoch 235, Loss: 22498.9639\nEpoch 236, Loss: 22527.7410\nEpoch 237, Loss: 22515.3842\nEpoch 238, Loss: 22506.0052\nEpoch 239, Loss: 22518.5188\nEpoch 240, Loss: 22498.1130\nEpoch 241, Loss: 22525.2499\nEpoch 242, Loss: 22497.5306\nEpoch 243, Loss: 22503.2707\nEpoch 244, Loss: 22500.4136\nEpoch 245, Loss: 22492.0763\nEpoch 246, Loss: 22511.3642\nEpoch 247, Loss: 22488.3698\nEpoch 248, Loss: 22495.7982\nEpoch 249, Loss: 22499.7157\nEpoch 250, Loss: 22526.4360\nEpoch 251, Loss: 22503.2600\nEpoch 252, Loss: 22473.7338\nEpoch 253, Loss: 22576.0246\nEpoch 254, Loss: 22502.4115\nEpoch 255, Loss: 22491.4190\nEpoch 256, Loss: 22496.0712\nEpoch 257, Loss: 22476.1478\nEpoch 258, Loss: 22471.6612\nEpoch 259, Loss: 22494.3420\nEpoch 260, Loss: 22454.5921\nEpoch 261, Loss: 22470.3841\nEpoch 262, Loss: 22469.2927\nEpoch 263, Loss: 22469.3364\nEpoch 264, Loss: 22475.6867\nEpoch 265, Loss: 22484.3528\nEpoch 266, Loss: 22471.2154\nEpoch 267, Loss: 22469.8689\nEpoch 268, Loss: 22495.4226\nEpoch 269, Loss: 22469.4510\nEpoch 270, Loss: 22455.2010\nEpoch 271, Loss: 22460.7913\nEpoch 272, Loss: 22466.4769\nEpoch 273, Loss: 22448.5984\nEpoch 274, Loss: 22457.0168\nEpoch 275, Loss: 22452.7579\nEpoch 276, Loss: 22441.7965\nEpoch 277, Loss: 22466.2512\nEpoch 278, Loss: 22452.6702\nEpoch 279, Loss: 22452.7848\nEpoch 280, Loss: 22431.7858\nEpoch 281, Loss: 22444.7865\nEpoch 282, Loss: 22458.6847\nEpoch 283, Loss: 22455.4706\nEpoch 284, Loss: 22441.3799\nEpoch 285, Loss: 22469.0456\nEpoch 286, Loss: 22444.3419\nEpoch 287, Loss: 22460.5773\nEpoch 288, Loss: 22449.8658\nEpoch 289, Loss: 22469.1854\nEpoch 290, Loss: 22423.0345\nEpoch 291, Loss: 22430.9868\nEpoch 292, Loss: 22433.0299\nEpoch 293, Loss: 22436.4528\nEpoch 294, Loss: 22446.3620\nEpoch 295, Loss: 22428.5758\nEpoch 296, Loss: 22415.9483\nEpoch 297, Loss: 22453.3998\nEpoch 298, Loss: 22420.0797\nEpoch 299, Loss: 22440.0116\nEpoch 300, Loss: 22421.2825\nEpoch 301, Loss: 22442.9082\nEpoch 302, Loss: 22469.9084\nEpoch 303, Loss: 22440.9431\nEpoch 304, Loss: 22412.8321\nEpoch 305, Loss: 22450.7394\nEpoch 306, Loss: 22409.6067\nEpoch 307, Loss: 22417.2129\nEpoch 308, Loss: 22431.6119\nEpoch 309, Loss: 22416.4427\nEpoch 310, Loss: 22429.4165\nEpoch 311, Loss: 22439.4394\nEpoch 312, Loss: 22411.5534\nEpoch 313, Loss: 22389.6109\nEpoch 314, Loss: 22416.2800\nEpoch 315, Loss: 22416.8475\nEpoch 316, Loss: 22393.3030\nEpoch 317, Loss: 22408.7559\nEpoch 318, Loss: 22410.7058\nEpoch 319, Loss: 22418.4204\nEpoch 320, Loss: 22395.5814\nEpoch 321, Loss: 22407.1800\nEpoch 322, Loss: 22422.6597\nEpoch 323, Loss: 22415.3641\nEpoch 324, Loss: 22398.1749\nEpoch 325, Loss: 22423.3311\nEpoch 326, Loss: 22377.4418\nEpoch 327, Loss: 22397.5627\nEpoch 328, Loss: 22406.4335\nEpoch 329, Loss: 22389.5220\nEpoch 330, Loss: 22378.8643\nEpoch 331, Loss: 22408.4844\nEpoch 332, Loss: 22419.0555\nEpoch 333, Loss: 22394.2646\nEpoch 334, Loss: 22396.5985\nEpoch 335, Loss: 22388.5286\nEpoch 336, Loss: 22396.6333\nEpoch 337, Loss: 22389.7453\nEpoch 338, Loss: 22399.3183\nEpoch 339, Loss: 22374.3348\nEpoch 340, Loss: 22406.5740\nEpoch 341, Loss: 22407.4425\nEpoch 342, Loss: 22374.3130\nEpoch 343, Loss: 22474.1333\nEpoch 344, Loss: 22374.3133\nEpoch 345, Loss: 22397.6469\nEpoch 346, Loss: 22380.8855\nEpoch 347, Loss: 22382.7277\nEpoch 348, Loss: 22368.3428\nEpoch 349, Loss: 22370.2644\nEpoch 350, Loss: 22386.0696\nEpoch 351, Loss: 22360.6487\nEpoch 352, Loss: 22393.7252\nEpoch 353, Loss: 22381.3536\nEpoch 354, Loss: 22380.0722\nEpoch 355, Loss: 22370.6400\nEpoch 356, Loss: 22387.4903\nEpoch 357, Loss: 22385.6021\nEpoch 358, Loss: 22386.6150\nEpoch 359, Loss: 22386.4151\nEpoch 360, Loss: 22373.4691\nEpoch 361, Loss: 22357.1685\nEpoch 362, Loss: 22373.0051\nEpoch 363, Loss: 22373.8148\nEpoch 364, Loss: 22369.6722\nEpoch 365, Loss: 22349.4563\nEpoch 366, Loss: 22393.9406\nEpoch 367, Loss: 22377.5879\nEpoch 368, Loss: 22368.7497\nEpoch 369, Loss: 22379.4615\nEpoch 370, Loss: 22369.3443\nEpoch 371, Loss: 22349.7180\nEpoch 372, Loss: 22356.8898\nEpoch 373, Loss: 22369.7680\nEpoch 374, Loss: 22369.2509\nEpoch 375, Loss: 22362.4627\nEpoch 376, Loss: 22343.8581\nEpoch 377, Loss: 22362.8309\nEpoch 378, Loss: 22374.2975\nEpoch 379, Loss: 22346.8029\nEpoch 380, Loss: 22365.7661\nEpoch 381, Loss: 22344.8537\nEpoch 382, Loss: 22341.9315\nEpoch 383, Loss: 22358.0867\nEpoch 384, Loss: 22345.4775\nEpoch 385, Loss: 22359.4405\nEpoch 386, Loss: 22365.2672\nEpoch 387, Loss: 22375.1329\nEpoch 388, Loss: 22371.4607\nEpoch 389, Loss: 22359.9122\nEpoch 390, Loss: 22352.6250\nEpoch 391, Loss: 22347.0871\nEpoch 392, Loss: 22359.2461\nEpoch 393, Loss: 22356.7284\nEpoch 394, Loss: 22359.7449\nEpoch 395, Loss: 22363.2582\nEpoch 396, Loss: 22359.0428\nEpoch 397, Loss: 22344.7522\nEpoch 398, Loss: 22341.3384\nEpoch 399, Loss: 22328.7430\nEpoch 400, Loss: 22367.0728\nEpoch 401, Loss: 22343.6296\nEpoch 402, Loss: 22323.6979\nEpoch 403, Loss: 22351.0653\nEpoch 404, Loss: 22374.0591\nEpoch 405, Loss: 22343.5270\nEpoch 406, Loss: 22334.5254\nEpoch 407, Loss: 22328.8673\nEpoch 408, Loss: 22335.3270\nEpoch 409, Loss: 22346.7469\nEpoch 410, Loss: 22334.2334\nEpoch 411, Loss: 22332.5178\nEpoch 412, Loss: 22341.9392\nEpoch 413, Loss: 22357.9398\nEpoch 414, Loss: 22348.1003\nEpoch 415, Loss: 22334.9101\nEpoch 416, Loss: 22353.9508\nEpoch 417, Loss: 22334.8654\nEpoch 418, Loss: 22363.9363\nEpoch 419, Loss: 22304.4200\nEpoch 420, Loss: 22339.1167\nEpoch 421, Loss: 22354.6665\nEpoch 422, Loss: 22337.0263\nEpoch 423, Loss: 22354.2895\nEpoch 424, Loss: 22346.9409\nEpoch 425, Loss: 22330.0257\nEpoch 426, Loss: 22341.6662\nEpoch 427, Loss: 22351.4399\nEpoch 428, Loss: 22332.1476\nEpoch 429, Loss: 22333.7957\nEpoch 430, Loss: 22320.7974\nEpoch 431, Loss: 22337.1231\nEpoch 432, Loss: 22317.9496\nEpoch 433, Loss: 22300.6533\nEpoch 434, Loss: 22337.4632\nEpoch 435, Loss: 22340.7325\nEpoch 436, Loss: 22343.2420\nEpoch 437, Loss: 22346.5938\nEpoch 438, Loss: 22337.8025\nEpoch 439, Loss: 22325.6485\nEpoch 440, Loss: 22320.2535\nEpoch 441, Loss: 22328.2945\nEpoch 442, Loss: 22333.2334\nEpoch 443, Loss: 22330.6340\nEpoch 444, Loss: 22348.5349\nEpoch 445, Loss: 22326.6625\nEpoch 446, Loss: 22335.6891\nEpoch 447, Loss: 22337.0031\nEpoch 448, Loss: 22309.7092\nEpoch 449, Loss: 22336.4572\nEpoch 450, Loss: 22327.4614\nEpoch 451, Loss: 22337.4220\nEpoch 452, Loss: 22349.4418\nEpoch 453, Loss: 22335.7991\nEpoch 454, Loss: 22316.9128\nEpoch 455, Loss: 22335.2138\nEpoch 456, Loss: 22326.1290\nEpoch 457, Loss: 22325.5078\nEpoch 458, Loss: 22342.0026\nEpoch 459, Loss: 22347.7472\nEpoch 460, Loss: 22345.8881\nEpoch 461, Loss: 22315.3766\nEpoch 462, Loss: 22343.0320\nEpoch 463, Loss: 22322.5763\nEpoch 464, Loss: 22311.7119\nEpoch 465, Loss: 22338.5006\nEpoch 466, Loss: 22297.9474\nEpoch 467, Loss: 22332.0115\nEpoch 468, Loss: 22319.5333\nEpoch 469, Loss: 22318.7008\nEpoch 470, Loss: 22317.6068\nEpoch 471, Loss: 22335.5877\nEpoch 472, Loss: 22293.3712\nEpoch 473, Loss: 22301.6915\nEpoch 474, Loss: 22304.3298\nEpoch 475, Loss: 22310.3685\nEpoch 476, Loss: 22309.5256\nEpoch 477, Loss: 22306.8486\nEpoch 478, Loss: 22331.9652\nEpoch 479, Loss: 22305.8983\nEpoch 480, Loss: 22310.0093\nEpoch 481, Loss: 22305.6292\nEpoch 482, Loss: 22317.4146\nEpoch 483, Loss: 22332.4779\nEpoch 484, Loss: 22302.3285\nEpoch 485, Loss: 22316.4191\nEpoch 486, Loss: 22293.6334\nEpoch 487, Loss: 22297.3786\nEpoch 488, Loss: 22326.5379\nEpoch 489, Loss: 22306.3005\nEpoch 490, Loss: 22286.8501\nEpoch 491, Loss: 22307.8223\nEpoch 492, Loss: 22284.9308\nEpoch 493, Loss: 22310.4722\nEpoch 494, Loss: 22308.1819\nEpoch 495, Loss: 22319.4252\nEpoch 496, Loss: 22312.7045\nEpoch 497, Loss: 22295.0547\nEpoch 498, Loss: 22306.6617\nEpoch 499, Loss: 22303.7585\nEpoch 500, Loss: 22315.6825\n\nAccuracy: 0.8324\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.86      0.84      0.85       607\n           1       0.92      0.92      0.92       627\n           2       0.81      0.76      0.78       626\n           3       0.75      0.82      0.79       640\n\n    accuracy                           0.83      2500\n   macro avg       0.83      0.83      0.83      2500\nweighted avg       0.83      0.83      0.83      2500\n\n","output_type":"stream"},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"['prodlda_bow_vectorizer.pkl']"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"!pip install gradio","metadata":{"execution":{"iopub.status.busy":"2025-05-28T09:41:37.318697Z","iopub.execute_input":"2025-05-28T09:41:37.318989Z","iopub.status.idle":"2025-05-28T09:41:47.299474Z","shell.execute_reply.started":"2025-05-28T09:41:37.318967Z","shell.execute_reply":"2025-05-28T09:41:47.298571Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting gradio\n  Downloading gradio-5.31.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\nCollecting fastapi<1.0,>=0.115.2 (from gradio)\n  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting gradio-client==1.10.1 (from gradio)\n  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting groovy~=0.1 (from gradio)\n  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.1)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\nRequirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\nRequirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\nRequirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.18 (from gradio)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\nCollecting ruff>=0.9.3 (from gradio)\n  Downloading ruff-0.11.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\nCollecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\nCollecting uvicorn>=0.14.0 (from gradio)\n  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\nRequirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading gradio-5.31.0-py3-none-any.whl (54.2 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\nDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading ruff-0.11.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\nDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\nInstalling collected packages: uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, starlette, safehttpx, gradio-client, fastapi, gradio\nSuccessfully installed fastapi-0.115.12 ffmpy-0.5.0 gradio-5.31.0 gradio-client-1.10.1 groovy-0.1.2 python-multipart-0.0.20 ruff-0.11.11 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import gradio as gr\nimport numpy as np\nimport torch\nimport joblib\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import LabelEncoder\nimport torch.nn as nn\n\n# ƒê·ªãnh nghƒ©a l·∫°i SimpleNN v√† ProdLDA n·∫øu c·∫ßn\nclass SimpleNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        self.relu3 = nn.ReLU()\n        self.output = nn.Linear(hidden_dim, output_dim)\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu1(x)\n        x = self.fc2(x)\n        x = self.relu2(x)\n        x = self.fc3(x)\n        x = self.relu3(x)\n        x = self.output(x)\n        return x\n\nclass ProdLDA(nn.Module):\n    def __init__(self, vocab_size, num_topics=50, en_units=200, dropout=0.4):\n        super().__init__()\n        self.num_topics = num_topics\n        import numpy as np\n        self.a = 1 * np.ones((1, num_topics)).astype(np.float32)\n        self.mu2 = nn.Parameter(torch.as_tensor((np.log(self.a).T - np.mean(np.log(self.a), 1)).T))\n        self.var2 = nn.Parameter(torch.as_tensor((((1.0 / self.a) * (1 - (2.0 / num_topics))).T + (1.0 / (num_topics * num_topics)) * np.sum(1.0 / self.a, 1)).T))\n        self.mu2.requires_grad = False\n        self.var2.requires_grad = False\n        self.fc11 = nn.Linear(vocab_size, en_units)\n        self.fc12 = nn.Linear(en_units, en_units)\n        self.fc21 = nn.Linear(en_units, num_topics)\n        self.fc22 = nn.Linear(en_units, num_topics)\n        self.mean_bn = nn.BatchNorm1d(num_topics, eps=0.001, momentum=0.001, affine=True)\n        self.mean_bn.weight.data.copy_(torch.ones(num_topics))\n        self.mean_bn.weight.requires_grad = False\n        self.logvar_bn = nn.BatchNorm1d(num_topics, eps=0.001, momentum=0.001, affine=True)\n        self.logvar_bn.weight.data.copy_(torch.ones(num_topics))\n        self.logvar_bn.weight.requires_grad = False\n        self.decoder_bn = nn.BatchNorm1d(vocab_size, eps=0.001, momentum=0.001, affine=True)\n        self.decoder_bn.weight.data.copy_(torch.ones(vocab_size))\n        self.decoder_bn.weight.requires_grad = False\n        self.fc1_drop = nn.Dropout(dropout)\n        self.theta_drop = nn.Dropout(dropout)\n        self.fcd1 = nn.Linear(num_topics, vocab_size, bias=False)\n        nn.init.xavier_uniform_(self.fcd1.weight)\n    def get_theta(self, x):\n        mu, logvar = self.encode(x)\n        import torch.nn.functional as F\n        z = self.reparameterize(mu, logvar)\n        theta = F.softmax(z, dim=1)\n        theta = self.theta_drop(theta)\n        if self.training:\n            return theta, mu, logvar\n        else:\n            return theta\n    def reparameterize(self, mu, logvar):\n        if self.training:\n            std = torch.exp(0.5 * logvar)\n            eps = torch.randn_like(std)\n            return mu + (eps * std)\n        else:\n            return mu\n    def encode(self, x):\n        import torch.nn.functional as F\n        e1 = F.softplus(self.fc11(x))\n        e1 = F.softplus(self.fc12(e1))\n        e1 = self.fc1_drop(e1)\n        return self.mean_bn(self.fc21(e1)), self.logvar_bn(self.fc22(e1))\n    def forward(self, x):\n        theta, mu, logvar = self.get_theta(x)\n        return theta\n\nlabel_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load c√°c m√¥ h√¨nh v√† vectorizer ƒë√£ l∆∞u\nnn_ckpt = torch.load('nn_model.pth', map_location=device, weights_only=False)\nnn_vectorizer = nn_ckpt['vectorizer']\nnn_label_encoder = nn_ckpt['label_encoder']\ninput_dim = len(nn_vectorizer.get_feature_names_out())\nhidden_dim = 128\noutput_dim = len(nn_label_encoder.classes_)\nnn_model = SimpleNN(input_dim, hidden_dim, output_dim)\nnn_model.load_state_dict(nn_ckpt['model_state_dict'])\nnn_model = nn_model.to(device)\nnn_model.eval()\n\nnb_model = joblib.load('nb_model.pkl')\nnb_vectorizer = joblib.load('nb_vectorizer.pkl')\nnb_label_encoder = joblib.load('nb_label_encoder.pkl')\n\nlr_model = joblib.load('lr_model.pkl')\nlr_vectorizer = joblib.load('lr_vectorizer.pkl')\nlr_label_encoder = joblib.load('lr_label_encoder.pkl')\n\nsvc_model = joblib.load('svc_model.pkl')\nsvc_vectorizer = joblib.load('svc_vectorizer.pkl')\nsvc_label_encoder = joblib.load('svc_label_encoder.pkl')\n\nsgd_model = joblib.load('sgd_model.pkl')\nsgd_vectorizer = joblib.load('sgd_vectorizer.pkl')\nsgd_label_encoder = joblib.load('sgd_label_encoder.pkl')\n\nrf_model = joblib.load('rf_model.pkl')\nrf_vectorizer = joblib.load('rf_vectorizer.pkl')\nrf_label_encoder = joblib.load('rf_label_encoder.pkl')\n\nprodlda_vectorizer = joblib.load('prodlda_bow_vectorizer.pkl')\nprodlda_svm = joblib.load('prodlda_svm.pkl')\nprodlda_model = ProdLDA(vocab_size=len(prodlda_vectorizer.vocabulary_), num_topics=50)\nprodlda_model.load_state_dict(torch.load('prodlda.pth', map_location=device))\nprodlda_model = prodlda_model.to(device)\nprodlda_model.eval()\n\ndef prodlda_infer(text):\n    bow = prodlda_vectorizer.transform([text])\n    input_tensor = torch.FloatTensor(bow.toarray()).to(device)\n    with torch.no_grad():\n        theta = prodlda_model.get_theta(input_tensor)\n        if isinstance(theta, tuple):\n            theta = theta[0]\n        pred = prodlda_svm.predict(theta.cpu().numpy())[0]\n    return label_names[int(pred)]\n\n# Cluster centroids (d√πng tr·ª±c ti·∫øp text_train, label_train ƒë√£ c√≥)\n# Load cluster parameters\ncluster_vectorizer = joblib.load('cluster_vectorizer.pkl')\ncluster_centroids = joblib.load('cluster_centroids.pkl')\ndef cluster_infer(text):\n    vec = cluster_vectorizer.transform([text])\n    similarities = cosine_similarity(vec, cluster_centroids)\n    pred = np.argmax(similarities, axis=1)[0]\n    return label_names[int(pred)]\n\ndef infer(model_name, input_text):\n    if model_name == \"Neural Network\":\n        vec = nn_vectorizer.transform([input_text])\n        input_tensor = torch.FloatTensor(vec.toarray()).to(device)\n        with torch.no_grad():\n            outputs = nn_model(input_tensor)\n            _, pred = torch.max(outputs, 1)\n            pred = pred.item()\n        return label_names[int(pred)]\n    elif model_name == \"Naive Bayes\":\n        vec = nb_vectorizer.transform([input_text])\n        pred = nb_model.predict(vec)[0]\n        # pred l√† index, n√™n d√πng label_names\n        return label_names[int(pred)]\n    elif model_name == \"Logistic Regression\":\n        vec = lr_vectorizer.transform([input_text])\n        pred = lr_model.predict(vec)[0]\n        return label_names[int(pred)]\n    elif model_name == \"SVM\":\n        vec = svc_vectorizer.transform([input_text])\n        pred = svc_model.predict(vec)[0]\n        return label_names[int(pred)]\n    elif model_name == \"SGD\":\n        vec = sgd_vectorizer.transform([input_text])\n        pred = sgd_model.predict(vec)[0]\n        return label_names[int(pred)]\n    elif model_name == \"Random Forest\":\n        vec = rf_vectorizer.transform([input_text])\n        pred = rf_model.predict(vec)[0]\n        return label_names[int(pred)]\n    elif model_name == \"Prod LDA\":\n        return prodlda_infer(input_text)\n    elif model_name == \"Cluster\":\n        return cluster_infer(input_text)\n    else:\n        return \"Unknown model\"\n\nmodel_choices = [\n    \"Neural Network\", \"Naive Bayes\", \"Logistic Regression\", \"SVM\", \"SGD\", \"Random Forest\", \"Prod LDA\", \"Cluster\"\n]\n\ngr.Interface(\n    fn=infer,\n    inputs=[\n        gr.Dropdown(choices=model_choices, label=\"Select Model\"),\n        gr.Textbox(lines=4, label=\"Enter News Content\")\n    ],\n    outputs=gr.Label(label=\"Predicted Category\"),\n    title=\"AG News Text Classification\",\n    description=\"Choose a model and enter a news article to classify it into one of 4 categories: World, Sports, Business, Sci/Tech.\"\n).launch()","metadata":{"execution":{"iopub.status.busy":"2025-05-28T10:53:37.069485Z","iopub.execute_input":"2025-05-28T10:53:37.069753Z","iopub.status.idle":"2025-05-28T10:53:38.907282Z","shell.execute_reply.started":"2025-05-28T10:53:37.069733Z","shell.execute_reply":"2025-05-28T10:53:38.906557Z"},"trusted":true},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7863\nIt looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://3054e7223111dc8c61.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://3054e7223111dc8c61.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":53,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"rm /kaggle/working/archive.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T10:54:33.610825Z","iopub.execute_input":"2025-05-28T10:54:33.611394Z","iopub.status.idle":"2025-05-28T10:54:33.799147Z","shell.execute_reply.started":"2025-05-28T10:54:33.611373Z","shell.execute_reply":"2025-05-28T10:54:33.797858Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"!zip -r /kaggle/working/archive.zip /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T10:54:53.028985Z","iopub.execute_input":"2025-05-28T10:54:53.029237Z","iopub.status.idle":"2025-05-28T10:55:03.010578Z","shell.execute_reply.started":"2025-05-28T10:54:53.029213Z","shell.execute_reply":"2025-05-28T10:55:03.009935Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/state.db (deflated 85%)\n  adding: kaggle/working/prodlda_svm.pkl (deflated 53%)\n  adding: kaggle/working/nn_model.pth (deflated 16%)\n  adding: kaggle/working/nb_label_encoder.pkl (deflated 24%)\n  adding: kaggle/working/svc_model.pkl (deflated 31%)\n  adding: kaggle/working/sgd_model.pkl (deflated 5%)\n  adding: kaggle/working/svc_vectorizer.pkl (deflated 74%)\n  adding: kaggle/working/text_classifier.pth (deflated 16%)\n  adding: kaggle/working/.gradio/ (stored 0%)\n  adding: kaggle/working/.gradio/certificate.pem (deflated 24%)\n  adding: kaggle/working/rf_vectorizer.pkl (deflated 74%)\n  adding: kaggle/working/lr_vectorizer.pkl (deflated 74%)\n  adding: kaggle/working/lr_label_encoder.pkl (deflated 24%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n  adding: kaggle/working/rf_label_encoder.pkl (deflated 24%)\n  adding: kaggle/working/sgd_vectorizer.pkl (deflated 74%)\n  adding: kaggle/working/nb_vectorizer.pkl (deflated 74%)\n  adding: kaggle/working/ECRTM/ (stored 0%)\n  adding: kaggle/working/ECRTM/img/ (stored 0%)\n  adding: kaggle/working/ECRTM/img/annotation.png (deflated 3%)\n  adding: kaggle/working/ECRTM/ECRTM/ (stored 0%)\n  adding: kaggle/working/ECRTM/ECRTM/utils/ (stored 0%)\n  adding: kaggle/working/ECRTM/ECRTM/utils/eva/ (stored 0%)\n  adding: kaggle/working/ECRTM/ECRTM/utils/eva/cluster.py (deflated 57%)\n  adding: kaggle/working/ECRTM/ECRTM/utils/eva/TD.py (deflated 51%)\n  adding: kaggle/working/ECRTM/ECRTM/utils/data/ (stored 0%)\n  adding: kaggle/working/ECRTM/ECRTM/utils/data/TextData.py (deflated 68%)\n  adding: kaggle/working/ECRTM/ECRTM/utils/data/file_utils.py (deflated 65%)\n  adding: kaggle/working/ECRTM/ECRTM/models/ (stored 0%)\n  adding: kaggle/working/ECRTM/ECRTM/models/ECR.py (deflated 61%)\n  adding: kaggle/working/ECRTM/ECRTM/models/ECRTM.py (deflated 71%)\n  adding: kaggle/working/ECRTM/ECRTM/scripts/ (stored 0%)\n  adding: kaggle/working/ECRTM/ECRTM/scripts/run.sh (deflated 41%)\n  adding: kaggle/working/ECRTM/ECRTM/scripts/preprocess.sh (deflated 54%)\n  adding: kaggle/working/ECRTM/ECRTM/scripts/eva.sh (deflated 39%)\n  adding: kaggle/working/ECRTM/ECRTM/run.py (deflated 63%)\n  adding: kaggle/working/ECRTM/ECRTM/preprocess/ (stored 0%)\n  adding: kaggle/working/ECRTM/ECRTM/preprocess/download_20ng.py (deflated 58%)\n  adding: kaggle/working/ECRTM/ECRTM/preprocess/download_imdb.py (deflated 66%)\n  adding: kaggle/working/ECRTM/ECRTM/preprocess/preprocess.py (deflated 72%)\n  adding: kaggle/working/ECRTM/ECRTM/configs/ (stored 0%)\n  adding: kaggle/working/ECRTM/ECRTM/configs/model/ (stored 0%)\n  adding: kaggle/working/ECRTM/ECRTM/configs/model/ECRTM_YahooAnswer.yaml (deflated 23%)\n  adding: kaggle/working/ECRTM/ECRTM/configs/model/ECRTM_AGNews.yaml (deflated 22%)\n  adding: kaggle/working/ECRTM/ECRTM/configs/model/ECRTM_IMDB.yaml (deflated 24%)\n  adding: kaggle/working/ECRTM/ECRTM/configs/model/ECRTM_20NG.yaml (deflated 23%)\n  adding: kaggle/working/ECRTM/ECRTM/runners/ (stored 0%)\n  adding: kaggle/working/ECRTM/ECRTM/runners/Runner.py (deflated 66%)\n  adding: kaggle/working/ECRTM/README.md (deflated 49%)\n  adding: kaggle/working/ECRTM/.git/ (stored 0%)\n  adding: kaggle/working/ECRTM/.git/refs/ (stored 0%)\n  adding: kaggle/working/ECRTM/.git/refs/heads/ (stored 0%)\n  adding: kaggle/working/ECRTM/.git/refs/heads/master (stored 0%)\n  adding: kaggle/working/ECRTM/.git/refs/remotes/ (stored 0%)\n  adding: kaggle/working/ECRTM/.git/refs/remotes/origin/ (stored 0%)\n  adding: kaggle/working/ECRTM/.git/refs/remotes/origin/HEAD (stored 0%)\n  adding: kaggle/working/ECRTM/.git/packed-refs (deflated 11%)\n  adding: kaggle/working/ECRTM/.git/index (deflated 51%)\n  adding: kaggle/working/ECRTM/.git/HEAD (stored 0%)\n  adding: kaggle/working/ECRTM/.git/logs/ (stored 0%)\n  adding: kaggle/working/ECRTM/.git/logs/refs/ (stored 0%)\n  adding: kaggle/working/ECRTM/.git/logs/refs/heads/ (stored 0%)\n  adding: kaggle/working/ECRTM/.git/logs/refs/heads/master (deflated 25%)\n  adding: kaggle/working/ECRTM/.git/logs/refs/remotes/ (stored 0%)\n  adding: kaggle/working/ECRTM/.git/logs/refs/remotes/origin/ (stored 0%)\n  adding: kaggle/working/ECRTM/.git/logs/refs/remotes/origin/HEAD (deflated 25%)\n  adding: kaggle/working/ECRTM/.git/logs/HEAD (deflated 25%)\n  adding: kaggle/working/ECRTM/.git/hooks/ (stored 0%)\n  adding: kaggle/working/ECRTM/.git/hooks/pre-merge-commit.sample (deflated 39%)\n  adding: kaggle/working/ECRTM/.git/hooks/post-update.sample (deflated 27%)\n  adding: kaggle/working/ECRTM/.git/hooks/pre-push.sample (deflated 49%)\n  adding: kaggle/working/ECRTM/.git/hooks/commit-msg.sample (deflated 44%)\n  adding: kaggle/working/ECRTM/.git/hooks/pre-applypatch.sample (deflated 38%)\n  adding: kaggle/working/ECRTM/.git/hooks/applypatch-msg.sample (deflated 42%)\n  adding: kaggle/working/ECRTM/.git/hooks/update.sample (deflated 68%)\n  adding: kaggle/working/ECRTM/.git/hooks/pre-rebase.sample (deflated 59%)\n  adding: kaggle/working/ECRTM/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n  adding: kaggle/working/ECRTM/.git/hooks/push-to-checkout.sample (deflated 55%)\n  adding: kaggle/working/ECRTM/.git/hooks/pre-commit.sample (deflated 45%)\n  adding: kaggle/working/ECRTM/.git/hooks/pre-receive.sample (deflated 40%)\n  adding: kaggle/working/ECRTM/.git/hooks/fsmonitor-watchman.sample (deflated 62%)\n  adding: kaggle/working/ECRTM/.git/config (deflated 30%)\n  adding: kaggle/working/ECRTM/.git/description (deflated 14%)\n  adding: kaggle/working/ECRTM/.git/info/ (stored 0%)\n  adding: kaggle/working/ECRTM/.git/info/exclude (deflated 28%)\n  adding: kaggle/working/ECRTM/.git/objects/ (stored 0%)\n  adding: kaggle/working/ECRTM/.git/objects/pack/ (stored 0%)\n  adding: kaggle/working/ECRTM/.git/objects/pack/pack-3fa3df3ffbf804898a9a6c3c7695eaab53f4efd6.pack (deflated 0%)\n  adding: kaggle/working/ECRTM/.git/objects/pack/pack-3fa3df3ffbf804898a9a6c3c7695eaab53f4efd6.idx (deflated 26%)\n  adding: kaggle/working/ECRTM/data/ (stored 0%)\n  adding: kaggle/working/ECRTM/data/stopwords/ (stored 0%)\n  adding: kaggle/working/ECRTM/data/stopwords/mallet_stopwords.txt (deflated 56%)\n  adding: kaggle/working/ECRTM/data/stopwords/snowball_stopwords.txt (deflated 53%)\n  adding: kaggle/working/ECRTM/data/stopwords/custom_stopwords.txt (deflated 69%)\n  adding: kaggle/working/ECRTM/data/20NG/ (stored 0%)\n  adding: kaggle/working/ECRTM/data/20NG/train_labels.txt (deflated 70%)\n  adding: kaggle/working/ECRTM/data/20NG/test_bow.npz (deflated 1%)\n  adding: kaggle/working/ECRTM/data/20NG/word_embeddings.npz (deflated 1%)\n  adding: kaggle/working/ECRTM/data/20NG/test_labels.txt (deflated 70%)\n  adding: kaggle/working/ECRTM/data/20NG/test_texts.txt (deflated 67%)\n  adding: kaggle/working/ECRTM/data/20NG/vocab.txt (deflated 62%)\n  adding: kaggle/working/ECRTM/data/20NG/train_bow.npz (deflated 1%)\n  adding: kaggle/working/ECRTM/data/20NG/train_texts.txt (deflated 67%)\n  adding: kaggle/working/ECRTM/data/IMDB/ (stored 0%)\n  adding: kaggle/working/ECRTM/data/IMDB/train_labels.txt (deflated 100%)\n  adding: kaggle/working/ECRTM/data/IMDB/test_bow.npz (deflated 0%)\n  adding: kaggle/working/ECRTM/data/IMDB/word_embeddings.npz (deflated 1%)\n  adding: kaggle/working/ECRTM/data/IMDB/test_labels.txt (deflated 100%)\n  adding: kaggle/working/ECRTM/data/IMDB/test_texts.txt (deflated 65%)\n  adding: kaggle/working/ECRTM/data/IMDB/vocab.txt (deflated 62%)\n  adding: kaggle/working/ECRTM/data/IMDB/train_bow.npz (deflated 0%)\n  adding: kaggle/working/ECRTM/data/IMDB/train_texts.txt (deflated 65%)\n  adding: kaggle/working/ECRTM/data/YahooAnswer/ (stored 0%)\n  adding: kaggle/working/ECRTM/data/YahooAnswer/train_labels.txt (deflated 70%)\n  adding: kaggle/working/ECRTM/data/YahooAnswer/test_bow.npz (deflated 1%)\n  adding: kaggle/working/ECRTM/data/YahooAnswer/word_embeddings.npz (deflated 1%)\n  adding: kaggle/working/ECRTM/data/YahooAnswer/test_labels.txt (deflated 69%)\n  adding: kaggle/working/ECRTM/data/YahooAnswer/test_texts.txt (deflated 64%)\n  adding: kaggle/working/ECRTM/data/YahooAnswer/vocab.txt (deflated 62%)\n  adding: kaggle/working/ECRTM/data/YahooAnswer/train_bow.npz (deflated 0%)\n  adding: kaggle/working/ECRTM/data/YahooAnswer/train_texts.txt (deflated 64%)\n  adding: kaggle/working/ECRTM/data/AGNews/ (stored 0%)\n  adding: kaggle/working/ECRTM/data/AGNews/train_labels.txt (deflated 82%)\n  adding: kaggle/working/ECRTM/data/AGNews/test_bow.npz (deflated 1%)\n  adding: kaggle/working/ECRTM/data/AGNews/word_embeddings.npz (deflated 1%)\n  adding: kaggle/working/ECRTM/data/AGNews/test_labels.txt (deflated 80%)\n  adding: kaggle/working/ECRTM/data/AGNews/test_texts.txt (deflated 64%)\n  adding: kaggle/working/ECRTM/data/AGNews/vocab.txt (deflated 61%)\n  adding: kaggle/working/ECRTM/data/AGNews/train_bow.npz (deflated 0%)\n  adding: kaggle/working/ECRTM/data/AGNews/train_texts.txt (deflated 65%)\n  adding: kaggle/working/cluster_vectorizer.pkl (deflated 74%)\n  adding: kaggle/working/svc_label_encoder.pkl (deflated 24%)\n  adding: kaggle/working/sgd_label_encoder.pkl (deflated 24%)\n  adding: kaggle/working/rf_model.pkl (deflated 83%)\n  adding: kaggle/working/prodlda.pth (deflated 8%)\n  adding: kaggle/working/cluster_centroids.pkl (deflated 25%)\n  adding: kaggle/working/prodlda_bow_vectorizer.pkl (deflated 50%)\n  adding: kaggle/working/nb_model.pkl (deflated 25%)\n  adding: kaggle/working/lr_model.pkl (deflated 4%)\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}